{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "from collections import deque, namedtuple\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "# w2v required\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# DQN required\n",
    "import argparse\n",
    "import numpy as np\n",
    "# import logging\n",
    "# from matplotlib import animation # will be needed for rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All global variables needed\n",
    "Since some or all of these variables are needed for each cell below, it's difficult to put these inside the main function.\\\n",
    "Perhaps, using some args method might work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate custom map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# Function to generate custom map\n",
    "# ============================================================================================\n",
    "def make_env(env_name, env_dim = 4, seed = 42, stochastic = False):\n",
    "    env = gym.make(env_name, desc=generate_random_map(size=env_dim, seed=seed), \n",
    "                   is_slippery = stochastic, render_mode = 'rgb_array')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  16\n",
      "Action space:  4\n",
      "Folder 'mdp/runs_frozen' already exists.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "env_name: str\n",
    "env_dim: int --> Dimension of the game: 4x4 or 8x8\n",
    "seed\n",
    "stochastic = boolean --> Whether we use is_slippery = True or False\n",
    " '''\n",
    "env_name = \"FrozenLake-v1\"\n",
    "env_dim = 4\n",
    "stochastic = False\n",
    "seed = 42\n",
    "gamma = 0.99 # discount factor in Q computation\n",
    "alpha = 0.1 # learning rate in the table\n",
    "num_episodes_q_table = 100_000\n",
    "convergence_threshold = 1e-4\n",
    "epsilon_start = 1\n",
    "epsilon_decay_q_table = 0.99995\n",
    "epsilon_end = 0.01\n",
    "check_env_details = True\n",
    "\n",
    "# Creating the environment\n",
    "env = make_env(env_name=env_name, env_dim=env_dim, seed = seed, stochastic=stochastic)\n",
    "state_dim = env.observation_space.n\n",
    "action_dim = env.action_space.n\n",
    "print(\"State space: \", env.observation_space.n)\n",
    "print(\"Action space: \", env.action_space.n)\n",
    "\n",
    "# state and trajectories related variables\n",
    "num_episodes_trajectories = 10_000\n",
    "num_states = state_dim\n",
    "num_actions = action_dim\n",
    "max_eps_len = 100\n",
    "modified = \"medium\" # perfect - for perfect trajectories, random - for purely random trajectories or \"medium\" for the combined trajs\n",
    "\n",
    "# w2v related variables\n",
    "''' Potential values for embedding dimensions = {4, 8, 12, 16, 20, 32, 64} '''\n",
    "# w2v hyperparameters\n",
    "embed_dim = 32\n",
    "window_size = 2\n",
    "batch_size = 16\n",
    "w2v_epochs = 60\n",
    "w2v_lr = 0.01\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DQN variables\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--env\", default=\"FrozenLake-v1\")          # Gymnasium environment name # Default = MountainCar-v0\n",
    "parser.add_argument(\"--seed\", default=42, type=int)              # sets Gym, PyTorch and Numpy seeds\n",
    "parser.add_argument(\"--n-episodes\", default=2500, type=int)     # maximum number of training episodes\n",
    "parser.add_argument(\"--batch-size\", default=64, type=int)       # training batch size\n",
    "parser.add_argument(\"--discount\", default=0.99)                 # discount factor\n",
    "parser.add_argument(\"--lr\", default=5e-4)                       # learning rate\n",
    "parser.add_argument(\"--tau\", default=0.001)                     # soft update of target network\n",
    "parser.add_argument(\"--max-size\", default=int(1e5),type=int)    # experience replay buffer length\n",
    "parser.add_argument(\"--update-freq\", default=4, type=int)       # update frequency of target network\n",
    "parser.add_argument(\"--gpu-index\", default=0,type=int)\t\t      # GPU index\n",
    "parser.add_argument(\"--max-esp-len\", default=100, type=int)    # maximum time of an episode\n",
    "#exploration strategy\n",
    "parser.add_argument(\"--epsilon-start\", default=1)               # start value of epsilon\n",
    "parser.add_argument(\"--epsilon-end\", default=0.01)              # end value of epsilon\n",
    "parser.add_argument(\"--epsilon-decay\", default=0.995)           # decay value of epsilon\n",
    "parser.add_argument(\"--save-filename\", default = \"dqn_w2v_FrozenLake-v1\" )\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# Filename for saving purposes - common part for all files\n",
    "filename = f\"{env_name}_map_size_{env_dim}_stochastic_{stochastic}_seed_{seed}\"\n",
    "\n",
    "# This folder path will be used to store all the saved models and the associated data\n",
    "runs_folder_path = \"mdp/runs_frozen\"\n",
    "\n",
    "# Check if the folder exists\n",
    "if not os.path.exists(runs_folder_path):\n",
    "    # Create the folder\n",
    "    os.makedirs(runs_folder_path)\n",
    "    print(f\"Folder '{runs_folder_path}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{runs_folder_path}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# Q-Learning Agent class\n",
    "# ============================================================================================\n",
    "class QLearningAgent:\n",
    "    \"\"\"Q-learning agent.\"\"\"\n",
    "    def __init__(self, num_states, num_actions, gamma=0.99, epsilon=0.1, alpha=0.1):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = np.zeros((num_states, num_actions))  # Initialize Q-table\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            return random.randint(0, self.num_actions - 1)  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state, :])  # Exploit\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        \"\"\"Q-learning update rule.\"\"\"\n",
    "        best_next_action = np.argmax(self.q_table[next_state, :])  # Greedy action for next state\n",
    "        td_target = reward + self.gamma * self.q_table[next_state, best_next_action]\n",
    "        td_error = td_target - self.q_table[state, action]\n",
    "        self.q_table[state, action] += self.alpha * td_error  # Update Q-table\n",
    "\n",
    "    def get_optimal_policy(self):\n",
    "        \"\"\"Extracts the optimal policy after training.\"\"\"\n",
    "        return np.argmax(self.q_table, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train the model using Q-learning for Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# Function to train the model using Q-learning for Frozen Lake\n",
    "# ============================================================================================\n",
    "def run_tabular_q_frozen(env, agent, num_episodes=10, max_eps_len = 100, convergence_threshold=1e-4,\n",
    "                         epsilon_start = 1, epsilon_decay = 0.995, epsilon_end = 0.01, seed=42):\n",
    "    reward_curve = [] # this will store the moving avg of rewards\n",
    "    moving_window = deque(maxlen=100)\n",
    "    epsilon = epsilon_start\n",
    "    prev_q_table = np.copy(agent.q_table)  # Store old Q-table\n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state,_ = env.reset(seed=seed)\n",
    "        # print(f\"\\nIn episode {episode}, After reset initial state = {state} and epsilon = {epsilon}\")\n",
    "        curr_reward = 0\n",
    "        flag = False\n",
    "\n",
    "        for _ in range(max_eps_len):\n",
    "            action = agent.choose_action(state, epsilon)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.update_q_value(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            curr_reward += reward\n",
    "\n",
    "            # # Compute max Q-value change\n",
    "            # q_change = np.max(np.abs(agent.q_table - prev_q_table))\n",
    "            # prev_q_table = np.copy(agent.q_table)\n",
    "\n",
    "            # # Check Q-value convergence\n",
    "            # if q_change < convergence_threshold:\n",
    "            #     print(f\"Q-values converged at Episode {episode+1} with max Q-change: {q_change}\")\n",
    "            #     flag = True\n",
    "            #     break\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        # end while inside an episode\n",
    "        \n",
    "        # Epsilon decay performed at the end of each episode\n",
    "        epsilon *= epsilon_decay\n",
    "        epsilon = max(epsilon, epsilon_end)\n",
    "\n",
    "        # Appending the smoothened reward\n",
    "        moving_window.append(curr_reward)\n",
    "        reward_curve.append(np.mean(moving_window))\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            print(f\"Tabular Q: Episode {episode}: epsilon = {epsilon}, avg reward = {np.mean(moving_window)}\")\n",
    "        # end if\n",
    "\n",
    "        # if flag:\n",
    "        #     break\n",
    "    # end for num_episode\n",
    "\n",
    "    return agent.q_table, agent.get_optimal_policy(), reward_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Tabular Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = make_env(env_name=env_name, env_dim=env_dim, seed = seed, stochastic=stochastic)\n",
    "\n",
    "# Setting seeds\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "if check_env_details:\n",
    "    # Extract the environment description (grid layout)\n",
    "    lake_grid = env.unwrapped.desc  # Gets the grid representation\n",
    "\n",
    "    # Print state-to-symbol mapping\n",
    "    print(\"Frozen Lake Grid Layout:\")\n",
    "    for row in lake_grid:\n",
    "        print(\" \".join(row.astype(str)))\n",
    "\n",
    "    goal_state = None\n",
    "    rows, cols = lake_grid.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if lake_grid[i, j] == b'G':  # 'G' is stored as a byte-string\n",
    "                goal_state = i * cols + j  # Convert (row, col) to state number\n",
    "                break\n",
    "        # end for j\n",
    "    # end for i\n",
    "    print(f\"Goal State: {goal_state}\")\n",
    "# end if check_env\n",
    "\n",
    "state_dim = env.observation_space.n\n",
    "action_dim = env.action_space.n\n",
    "print(\"State space: \", env.observation_space.n)\n",
    "print(\"Action space: \", env.action_space.n)\n",
    "\n",
    "learner = QLearningAgent(num_states=state_dim, num_actions=action_dim, gamma=gamma\n",
    "                            , epsilon=epsilon_start, alpha=alpha) # Creating the learning Agent\n",
    "\n",
    "final_q_table, final_policy, reward_curve = run_tabular_q_frozen(\n",
    "                env, learner, num_episodes=num_episodes_q_table, max_eps_len=max_eps_len, convergence_threshold=convergence_threshold,\n",
    "                epsilon_start = epsilon_start, epsilon_decay = epsilon_decay_q_table, epsilon_end = epsilon_end, seed=seed)\n",
    "\n",
    "Val_f = np.max(final_q_table, axis=1)\n",
    "\n",
    "state = 0\n",
    "# Define action map\n",
    "action_map = {\n",
    "0: \"Left\",\n",
    "1: \"Down\",\n",
    "2: \"Right\",\n",
    "3: \"Up\"\n",
    "}\n",
    "print(\"State: Type -    V(s),    action taken\")\n",
    "lake_grid = env.unwrapped.desc  # Gets the grid representation\n",
    "for row in lake_grid:\n",
    "    for cell in row:\n",
    "        print(f\"     {state}:   {cell.decode('utf-8')} - {Val_f[state]:.2f}, {final_policy[state]}-->{action_map[final_policy[state]]}\")  # Convert byte to string\n",
    "        state += 1\n",
    "# assert False, \"c1\"\n",
    "\n",
    "# Print the final table and policy\n",
    "print(\"Final Q function: \", final_q_table)\n",
    "# print(\"Final Policy: \", final_policy)\n",
    "# print(\"Final Value function: \", Val_f)\n",
    "\n",
    "# # Plot heatmap of the Value function\n",
    "# plt.figure(figsize=(5,5))\n",
    "# plt.imshow(Val_f.reshape(4,4), cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "# for i in range(4):\n",
    "#     for j in range(4):\n",
    "#         plt.text(j, i, f\"{Val_f[i*4+j]:.2f}\", ha='center', va='center', color='black')\n",
    "\n",
    "\n",
    "# Plot the reward curve\n",
    "\n",
    "\n",
    "\n",
    "# Save the current Q-function\n",
    "save_model = f\"{runs_folder_path}/Q_table_{filename}.npy\"\n",
    "np.save(save_model, final_q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Trajectories\n",
    "Here we may need to ensure that the trajectories sufficiently explore each state.\\\n",
    "Thus, we may want to start from specific states when reseting the environment during trajectory collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(q_table, state, epsilon):\n",
    "    \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, num_actions - 1)  # Explore\n",
    "    else:\n",
    "        return np.argmax(q_table[state, :])  # Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.load(f\"{runs_folder_path}/Q_table_{filename}.npy\")\n",
    "# env = make_env(env_name=env_name, env_dim=env_dim, seed = seed, stochastic=stochastic)\n",
    "flag = False\n",
    "traj = []\n",
    "tuple_traj = False\n",
    "for e in tqdm(range(num_episodes_trajectories)):\n",
    "    if modified==\"perfect\":\n",
    "        _, _ = env.reset()\n",
    "        epsilon = 0.01\n",
    "        state = int(e%16)\n",
    "        if flag==False:\n",
    "            print(f\"We're dealing with {modified} trajs!\")\n",
    "            flag = True\n",
    "    # end if perfect\n",
    "\n",
    "    elif modified==\"random\":\n",
    "        epsilon = 1\n",
    "        if random.uniform(0, 1) < 0.1:\n",
    "            if random.uniform(0, 1) < 0.5:\n",
    "                state = 3\n",
    "            elif random.uniform(0, 1) < 0.65:\n",
    "                state = 6\n",
    "            else:\n",
    "                state = 7\n",
    "            # end if choosing state\n",
    "        else:\n",
    "            state, _ = env.reset(seed=seed)\n",
    "        \n",
    "        if flag==False:\n",
    "            print(f\"We're dealing with {modified} trajs!\")\n",
    "            flag = True\n",
    "    # end if random trajectories\n",
    "    \n",
    "    else:\n",
    "        # continue    \n",
    "        if e<0.1*num_episodes_trajectories:\n",
    "          epsilon = 1\n",
    "        elif e<0.2*num_episodes_trajectories:\n",
    "          epsilon = 0.8\n",
    "        elif e<0.3*num_episodes_trajectories:\n",
    "          epsilon = 0.6\n",
    "        elif e<0.4*num_episodes_trajectories:\n",
    "          epsilon = 0.4\n",
    "        elif e<0.5*num_episodes_trajectories:\n",
    "          epsilon = 0.2\n",
    "        elif e<0.6*num_episodes_trajectories:\n",
    "          epsilon = 0.1\n",
    "        elif e<0.7*num_episodes_trajectories:\n",
    "          epsilon = 0.05\n",
    "        else:\n",
    "          epsilon = 0.01\n",
    "        # end epsilon if\n",
    "        if random.uniform(0, 1) < 0.1:\n",
    "            state, _ = env.reset(seed=seed)\n",
    "            if random.uniform(0, 1) < 0.5:\n",
    "                state = 3\n",
    "            elif random.uniform(0, 1) < 0.65:\n",
    "                state = 6\n",
    "            else:\n",
    "                state = 7\n",
    "            # end if choosing state\n",
    "        else:\n",
    "            state, _ = env.reset(seed=seed)\n",
    "        \n",
    "        if flag==False:\n",
    "            print(f\"We're dealing with {modified} trajs!\")\n",
    "            flag = True\n",
    "        # end flag if\n",
    "    \n",
    "    if not tuple_traj:\n",
    "       traj.append('s_'+str(state))\n",
    "    '''\n",
    "    Action Stochasticity (is_slippery=True): The seed affects how the agent slips \n",
    "    (randomly moves instead of following the chosen action).\n",
    "    Random Hole Placement (if map is generated dynamically): If the map has random \n",
    "    elements, different seeds can affect the placement of H (holes).\n",
    "    '''\n",
    "    curr_reward = 0\n",
    "    rep_count = 0\n",
    "    for t in range(max_eps_len):\n",
    "      action = choose_action(q_table, state, epsilon) #To be implemented\n",
    "      n_state,reward,done,_,_ = env.step(action)\n",
    "      \n",
    "      # We store the current tuple\n",
    "      if tuple_traj:\n",
    "        traj.append((state, action, reward, n_state, done))\n",
    "      else:\n",
    "        temp = 's_'+str(n_state)\n",
    "        # traj.append('s_'+str(n_state))\n",
    "        traj.append(temp)\n",
    "\n",
    "      state = n_state\n",
    "      curr_reward += reward\n",
    "      if done:\n",
    "        if rep_count>=5: # This forces repetitions to occur when done becomes True thereby repeating ending states\n",
    "            if not tuple_traj:\n",
    "                if temp=='s_15':\n",
    "                    traj.append('s_'+str(16))\n",
    "                else:\n",
    "                    traj.append('s_'+str(17))\n",
    "            break\n",
    "        else:\n",
    "            if not tuple_traj:\n",
    "                traj.append(temp)\n",
    "        rep_count+=1\n",
    "    # end for\n",
    "# end for\n",
    "if tuple_traj:\n",
    "    save_file_name = f\"{runs_folder_path}/modified_tuple_trajectories_{filename}.npy\"\n",
    "    np.save(save_file_name, traj)\n",
    "else:\n",
    "    save_file_name = f\"{runs_folder_path}/modified_{modified}_trajectories_{filename}.npy\"\n",
    "    np.save(save_file_name, traj)\n",
    "print(f\"Trajectories Saved in {save_file_name}!\")\n",
    "\n",
    "# assert False, \"No w2v business here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_file = f\"{runs_folder_path}/modified_{modified}_trajectories_{filename}.npy\"\n",
    "print(\"The trajectories being used are: \", traj_file)\n",
    "\n",
    "text = np.load(traj_file)\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we define the functions for word2vec algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text): # Again we already have a vocabulary hence don't need to use this function directly\n",
    "    word_counts = Counter(text)\n",
    "    vocab = {word: i for i, word in enumerate(word_counts.keys())}\n",
    "    reverse_vocab = {i: word for word, i in vocab.items()}\n",
    "    return vocab, reverse_vocab, word_counts\n",
    "\n",
    "def generate_skipgram_pairs(text, window_size=2): # This function gives the word and context pairs.\n",
    "    # words = tokenize_text(text)\n",
    "    words = text\n",
    "    pairs = []\n",
    "    for i, target_word in enumerate(words):\n",
    "        window_start = max(i - window_size, 0)\n",
    "        window_end = min(i + window_size + 1, len(words))\n",
    "        for j in range(window_start, window_end):\n",
    "            if i != j:\n",
    "                pairs.append((words[i], words[j]))\n",
    "    return pairs\n",
    "\n",
    "vocab,_,wcounts = build_vocab(text)\n",
    "print(\"vocab: \", vocab)\n",
    "print(\"word counts: \", wcounts)\n",
    "# assert False, \"Checking the word counts!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# Classes: Word2vec Dataset creator and SkipGram model\n",
    "# ============================================================================================\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, text, vocab, window_size=2):\n",
    "        self.vocab = vocab\n",
    "        self.data = generate_skipgram_pairs(text, window_size)\n",
    "        self.vocab_size = len(vocab) # why is this needed here?\n",
    "\n",
    "    def __len__(self): # what does this function do?\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target, context = self.data[idx]\n",
    "        ''' Doesn't the above idx reflect the idx numbered pair instead in pairs instead of the pairs\n",
    "        corresponding to the word at idx? '''\n",
    "        target_idx = torch.tensor(self.vocab[target], dtype=torch.long)\n",
    "        context_idx = torch.tensor(self.vocab[context], dtype=torch.long)\n",
    "        # target_idx = self.vocab[target]\n",
    "        # context_idx = self.vocab[context]\n",
    "\n",
    "        return target_idx, context_idx\n",
    "    \n",
    "\n",
    "    # ============================================================================================\n",
    "# Class: SkipGram using softmax over entire vocabulary\n",
    "# ============================================================================================\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Input word embedding\n",
    "        self.in_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Output word embedding (used for context words)\n",
    "        self.out_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Initialize weights (better stability)\n",
    "        init_range = 0.5 / embedding_dim\n",
    "        self.in_embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.out_embedding.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, center_word_idx):\n",
    "        \"\"\"Compute word embeddings and softmax probabilities for the context words.\"\"\"\n",
    "        center_embed = F.relu(self.in_embedding(center_word_idx))  # Shape: (batch_size, embedding_dim)\n",
    "        scores = torch.matmul(center_embed, self.out_embedding.weight.T)  # Compute dot product\n",
    "        y_pred = torch.softmax(scores, dim=1)  # Apply softmax over output vocab\n",
    "        return y_pred\n",
    "\n",
    "    def get_word_vector(self, word_idx):\n",
    "        \"\"\"Return the learned embedding vector for a given word index.\"\"\"\n",
    "        return self.in_embedding(word_idx).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# Function: to train the w2v skipgram model\n",
    "# ============================================================================================\n",
    "\n",
    "def train_skipgram(model, data_loader, epochs=6, lr=0.01, device = device):\n",
    "    \"\"\"Train the SkipGram model using Adam optimizer.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()  # Cross-entropy for multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_loss = 0\n",
    "        for center_word_idx, context_word_idx in data_loader:\n",
    "            center_word_idx = center_word_idx.to(device)\n",
    "            context_word_idx = context_word_idx.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(center_word_idx)  # Forward pass\n",
    "            loss = criterion(y_pred, context_word_idx)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function for w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "#  Main function for calling the w2v agent\n",
    "# ============================================================================================\n",
    "# First we create the dataset and dataloader\n",
    "vocab, _, _ = build_vocab(text)\n",
    "dataset = Word2VecDataset(text, vocab, window_size)\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "model = SkipGram(vocab_size=len(vocab), embedding_dim=embed_dim).to(device)\n",
    "model = train_skipgram(model, dataloader, epochs = w2v_epochs, lr = w2v_lr)\n",
    "\n",
    "# ============================================================================================\n",
    "#  Saving the w2v generated vector embeddings as a dictionary\n",
    "# ============================================================================================\n",
    "word_embeddings = {}\n",
    "for word in vocab:\n",
    "    word_idx = torch.tensor([vocab[word]], dtype=torch.long).to(device)\n",
    "    updated_embedding = model.get_word_vector(word_idx)\n",
    "    # print(f\"Updated embedding for '{word}': {updated_embedding}\")\n",
    "    # Store the embedding in the dictionary\n",
    "    word_embeddings[word] = updated_embedding.flatten()  # Flatten to 1D array\n",
    "\n",
    "# Saving the w2v model\n",
    "save_w2v_file = f\"{runs_folder_path}/modified_{modified}_w2v_embed_dim_{embed_dim}_{filename}_epochs_{w2v_epochs}.npy\"\n",
    "np.save(save_w2v_file, word_embeddings)\n",
    "print(\"W2v vetors stored in: \", save_w2v_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# Similarity checking and visualizing \n",
    "# ============================================================================================\n",
    "# import numpy as np\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load saved embeddings\n",
    "word_embeddings = np.load(f\"{runs_folder_path}/modified_{modified}_w2v_embed_dim_{embed_dim}_{filename}_epochs_{w2v_epochs}.npy\", \n",
    "                          allow_pickle=True).item()\n",
    "\n",
    "# Convert to a NumPy array for fast computation\n",
    "words = list(word_embeddings.keys())\n",
    "vectors = np.array(list(word_embeddings.values()))\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(vectors)\n",
    "\n",
    "# Function to find top-N similar words\n",
    "def find_similar_words(target_word, top_n=5):\n",
    "    if target_word not in word_embeddings:\n",
    "        print(f\"Word '{target_word}' not found in vocabulary.\")\n",
    "        return []\n",
    "\n",
    "    # Get index of target word\n",
    "    target_idx = words.index(target_word)\n",
    "\n",
    "    # Get similarity scores for the target word\n",
    "    similarity_scores = cosine_sim_matrix[target_idx]\n",
    "\n",
    "    # Get top-N most similar words (excluding itself)\n",
    "    similar_indices = np.argsort(similarity_scores)[::-1][1:top_n+1]  # Sort in descending order\n",
    "\n",
    "    # Return words with their similarity scores\n",
    "    return [(words[i], similarity_scores[i]) for i in similar_indices]\n",
    "\n",
    "# Example usage\n",
    "for target_word in word_embeddings:\n",
    "    top_similar_words = find_similar_words(target_word, top_n=5)\n",
    "\n",
    "    print(f\"Top 5 words similar to '{target_word}':\")\n",
    "    for word, score in top_similar_words:\n",
    "        print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we need to ensure that the same states are sampled to compare the performance of w2v imbued DQN and Vanilla DQN. \n",
    "Thus we reset np, random and torch.manual_seed before starting both the following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN common set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining all classes\n",
    "We define the following classes:\n",
    "1. Experience Replay\n",
    "2. DQNAgent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "\t\"\"\"\n",
    "\tBased on the Replay Buffer implementation of TD3\n",
    "\tReference: https://github.com/sfujim/TD3/blob/master/utils.py\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, state_dim, action_dim,max_size,batch_size,gpu_index=0):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0\n",
    "\t\tself.size = 0\n",
    "\t\tself.state = np.zeros((max_size, state_dim))\n",
    "\t\tself.action = np.zeros((max_size, action_dim))\n",
    "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
    "\t\tself.reward = np.zeros((max_size, 1))\n",
    "\t\tself.done = np.zeros((max_size, 1))\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.device = torch.device('cuda', index=gpu_index) if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "\tdef add(self, state, action,reward,next_state, done):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.done[self.ptr] = done\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\tdef sample(self):\n",
    "\t\tind = np.random.randint(0, self.size, size=self.batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).long().to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.done[ind]).to(self.device)\n",
    "\t\t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class of QNetwork with w2v\n",
    "Here, the w2v input is given to a 2 layer NN. We remove the 64x64 layer since word vectors are already learnt and thus we don't require that layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2v_QNetwork(nn.Module):\n",
    "  \"\"\"\n",
    "  Q Network: designed to take state as input and give out Q values of actions as output\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    \"\"\"\n",
    "      state_dim (int): state dimenssion\n",
    "      action_dim (int): action dimenssion\n",
    "    \"\"\"\n",
    "    super(w2v_QNetwork, self).__init__()\n",
    "    self.l1 = nn.Linear(state_dim, 64)\n",
    "    # self.l2 = nn.Linear(64, 64)\n",
    "    self.l3 = nn.Linear(64, action_dim)\n",
    "\n",
    "  def forward(self, state):\n",
    "    q = F.relu(self.l1(state))\n",
    "    # q = F.relu(self.l2(q))\n",
    "    return self.l3(q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQNAgent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2v_DQNAgent():\n",
    "  def __init__(self,\n",
    "   embed_dim,\n",
    "   action_dim,\n",
    "   discount=0.99,\n",
    "   tau=1e-3,\n",
    "   lr=5e-4,\n",
    "   update_freq=4,\n",
    "   max_size=int(1e5),\n",
    "   batch_size=64,\n",
    "   gpu_index=0\n",
    "   ):\n",
    "    \"\"\"\n",
    "      state_size (int): dimension of each state\n",
    "      action_size (int): dimension of each action\n",
    "      discount (float): discount factor\n",
    "      tau (float): used to update q-target\n",
    "      lr (float): learning rate\n",
    "      update_freq (int): update frequency of target network\n",
    "      max_size (int): experience replay buffer size\n",
    "      batch_size (int): training batch size\n",
    "      gpu_index (int): GPU used for training\n",
    "    \"\"\"\n",
    "    self.embed_dim = embed_dim\n",
    "    self.action_dim = action_dim\n",
    "    self.discount = discount\n",
    "    self.tau = tau\n",
    "    self.lr = lr\n",
    "    self.update_freq = update_freq\n",
    "    self.batch_size = batch_size\n",
    "    self.device = torch.device('cuda', index=gpu_index) if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "    # Setting up the NNs\n",
    "    self.Q = w2v_QNetwork(embed_dim, action_dim).to(self.device)\n",
    "    self.Q_target = w2v_QNetwork(embed_dim, action_dim).to(self.device)\n",
    "    self.optimizer = optim.Adam(self.Q.parameters(), lr=self.lr)\n",
    "\n",
    "    # Experience Replay Buffer\n",
    "    self.memory = ExperienceReplay(embed_dim,1,max_size,self.batch_size,gpu_index)\n",
    "\n",
    "    self.t_train = 0\n",
    "\n",
    "  def step(self, state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    1. Adds (s,a,r,s') to the experience replay buffer, and updates the networks\n",
    "    2. Learns when the experience replay buffer has enough samples\n",
    "    3. Updates target netowork\n",
    "    \"\"\"\n",
    "    self.memory.add(state, action, reward, next_state, done)\n",
    "    self.t_train += 1\n",
    "\n",
    "    if self.memory.size > self.batch_size:\n",
    "      experiences = self.memory.sample()\n",
    "      self.learn(experiences, self.discount) #To be implemented\n",
    "\n",
    "    if (self.t_train % self.update_freq) == 0:\n",
    "      self.target_update(self.Q, self.Q_target, self.tau) #To be implemented\n",
    "\n",
    "  def select_action(self, state, epsilon):\n",
    "    \"\"\"\n",
    "    TODO: Complete this block to select action using epsilon greedy exploration\n",
    "    strategy\n",
    "    Input: state, epsilon\n",
    "    Return: Action\n",
    "    Return Type: int\n",
    "    \"\"\"\n",
    "    ###### TYPE YOUR CODE HERE ######\n",
    "    # We generate a random number between 0 and 1\n",
    "    rand_num = np.random.random()\n",
    "    state = torch.from_numpy(state).to(self.device)\n",
    "    a_opt = np.argmax(self.Q(state).cpu().detach().numpy())\n",
    "    if rand_num<epsilon:\n",
    "      a_list = [y for y in range(self.action_dim)]\n",
    "      # print('a_list = ', a_list)\n",
    "      a_list.remove(a_opt)\n",
    "      # print('a_list = ', a_list)\n",
    "      at = np.random.choice(np.array(a_list))\n",
    "      return (at)\n",
    "    else:\n",
    "      return(a_opt)\n",
    "    #################################\n",
    "\n",
    "  def learn(self, experiences, discount):\n",
    "    \"\"\"\n",
    "    TODO: Complete this block to update the Q-Network using the target network\n",
    "    1. Compute target using  self.Q_target ( target = r + discount * max_b [Q_target(s,b)] )\n",
    "    2. Compute Q(s,a) using self.Q\n",
    "    3. Compute MSE loss between step 1 and step 2\n",
    "    4. Update your network\n",
    "    Input: experiences consisting of states,actions,rewards,next_states and discount factor\n",
    "    Return: None\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    ###### TYPE YOUR CODE HERE ######\n",
    "    # Step 1:\n",
    "    target = rewards + discount * torch.max(self.Q_target(next_states), axis = 1, keepdim = True).values * (1 - dones)# change Q_target to Q\n",
    "    # Step 2:\n",
    "    Q_sa = torch.take_along_dim(self.Q(states), actions, dim = 1)\n",
    "    # Step 3:\n",
    "    loss = nn.MSELoss()\n",
    "    mse_loss = loss(target, Q_sa)\n",
    "    # Step 4:\n",
    "    self.optimizer.zero_grad()\n",
    "    mse_loss.backward()\n",
    "    self.optimizer.step()\n",
    "    #################################\n",
    "\n",
    "  def target_update(self, Q, Q_target, tau):\n",
    "    \"\"\"\n",
    "    TODO: Update the target network parameters (param_target) using current Q parameters (param_Q)\n",
    "    Perform the update using tau, this ensures that we do not change the target network drastically\n",
    "    1. param_target = tau * param_Q + (1 - tau) * param_target\n",
    "    Input: Q,Q_target,tau\n",
    "    Return: None\n",
    "    \"\"\"\n",
    "    ###### TYPE YOUR CODE HERE ######\n",
    "    for target_param, param in zip(Q_target.parameters(), Q.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    #################################\n",
    "\n",
    "  def get_optimal_policy(self, states):\n",
    "    \"\"\"Extracts the optimal policy after training.\"\"\"\n",
    "    policy = []\n",
    "    for state in states:\n",
    "      state = torch.from_numpy(state).to(self.device)\n",
    "      policy.append(np.argmax(self.Q(state).cpu().detach().numpy()))\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings loaded from:  mdp/runs_frozen/modified_medium_w2v_embed_dim_32_FrozenLake-v1_map_size_4_stochastic_False_seed_42_epochs_60.npy\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "word_embeddings = np.load(f\"{runs_folder_path}/modified_{modified}_w2v_embed_dim_{embed_dim}_{filename}_epochs_{w2v_epochs}.npy\", allow_pickle=True).item()\n",
    "print(\"Word embeddings loaded from: \", f\"{runs_folder_path}/modified_{modified}_w2v_embed_dim_{embed_dim}_{filename}_epochs_{w2v_epochs}.npy\")\n",
    "\n",
    "state_embeddings = []\n",
    "states = []\n",
    "words = list(word_embeddings.keys())\n",
    "#   print(word_embeddings['s_0'])\n",
    "for i in range(state_dim):\n",
    "  #  states.append(i)\n",
    "  if 's_'+str(i) in words:\n",
    "      state_embeddings.append(word_embeddings['s_'+str(i)])\n",
    "      # print(f\"s_{i}: {word_embeddings['s_'+str(i)]}\")\n",
    "  else:\n",
    "      temp = np.random.randn(embed_dim).astype(np.float32)\n",
    "      temp /= np.abs(np.max(temp))\n",
    "      state_embeddings.append(temp)\n",
    "      print(f\"s_{i} not present in the w2v model, hence random vector initialized\")\n",
    "      # print(f\"s_{i}: {temp}\")\n",
    "\n",
    "kwargs = {\n",
    "    \"embed_dim\":embed_dim,\n",
    "    \"action_dim\":action_dim,\n",
    "    \"discount\":gamma,\n",
    "    \"tau\":args.tau,\n",
    "    \"lr\":args.lr,\n",
    "    \"update_freq\":args.update_freq,\n",
    "    \"max_size\":args.max_size,\n",
    "    \"batch_size\":args.batch_size,\n",
    "    \"gpu_index\":args.gpu_index\n",
    "  }\n",
    "learner = w2v_DQNAgent(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings are loaded. Now we train dqn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 27/2500 [00:00<00:09, 261.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 0 Average Episodic Reward (over 100 episodes): 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 132/2500 [00:00<00:10, 229.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 100 Average Episodic Reward (over 100 episodes): 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 235/2500 [00:00<00:09, 235.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 200 Average Episodic Reward (over 100 episodes): 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 304/2500 [00:01<00:19, 115.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 300 Average Episodic Reward (over 100 episodes): 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 405/2500 [00:03<00:46, 45.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 400 Average Episodic Reward (over 100 episodes): 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 542/2500 [00:04<00:10, 178.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 500 Average Episodic Reward (over 100 episodes): 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 647/2500 [00:05<00:07, 235.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 600 Average Episodic Reward (over 100 episodes): 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 750/2500 [00:05<00:07, 249.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 700 Average Episodic Reward (over 100 episodes): 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 830/2500 [00:06<00:06, 252.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 800 Average Episodic Reward (over 100 episodes): 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 937/2500 [00:06<00:06, 254.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 900 Average Episodic Reward (over 100 episodes): 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 1044/2500 [00:06<00:05, 262.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 1000 Average Episodic Reward (over 100 episodes): 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 1125/2500 [00:07<00:05, 257.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 1100 Average Episodic Reward (over 100 episodes): 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 1232/2500 [00:07<00:04, 258.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 1200 Average Episodic Reward (over 100 episodes): 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 1340/2500 [00:08<00:04, 261.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 1300 Average Episodic Reward (over 100 episodes): 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1448/2500 [00:08<00:04, 259.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 1400 Average Episodic Reward (over 100 episodes): 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 1529/2500 [00:08<00:03, 264.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 1500 Average Episodic Reward (over 100 episodes): 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 1638/2500 [00:09<00:03, 262.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 1600 Average Episodic Reward (over 100 episodes): 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 1746/2500 [00:09<00:02, 263.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 1700 Average Episodic Reward (over 100 episodes): 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1827/2500 [00:09<00:02, 262.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 1800 Average Episodic Reward (over 100 episodes): 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1934/2500 [00:10<00:02, 257.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 1900 Average Episodic Reward (over 100 episodes): 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 2039/2500 [00:10<00:01, 256.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 2000 Average Episodic Reward (over 100 episodes): 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 2149/2500 [00:11<00:01, 262.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 2100 Average Episodic Reward (over 100 episodes): 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 2230/2500 [00:11<00:01, 263.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 2200 Average Episodic Reward (over 100 episodes): 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 2337/2500 [00:11<00:00, 249.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 2300 Average Episodic Reward (over 100 episodes): 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 2446/2500 [00:12<00:00, 262.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number 2400 Average Episodic Reward (over 100 episodes): 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:12<00:00, 199.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at:  mdp/runs_frozen/w2v_dqn_modified_medium_w2v_embed_dim_32_FrozenLake-v1_map_size_4_stochastic_False_seed_42_epochs_60.pt\n",
      "Final policy:  [1, 0, 2, 1, 1, 0, 1, 1, 2, 2, 1, 1, 3, 0, 2, 0]\n",
      "State: Type -    action taken\n",
      "     0:   S - 1-->Down\n",
      "     1:   F - 0-->Left\n",
      "     2:   H - 2-->Right\n",
      "     3:   F - 1-->Down\n",
      "     4:   F - 1-->Down\n",
      "     5:   H - 0-->Left\n",
      "     6:   F - 1-->Down\n",
      "     7:   F - 1-->Down\n",
      "     8:   F - 2-->Right\n",
      "     9:   F - 2-->Right\n",
      "     10:   F - 1-->Down\n",
      "     11:   H - 1-->Down\n",
      "     12:   F - 3-->Up\n",
      "     13:   H - 0-->Left\n",
      "     14:   F - 2-->Right\n",
      "     15:   G - 0-->Left\n",
      "It was successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# one_hot_state = state_embeddings\n",
    "print(\"Embeddings are loaded. Now we train dqn\")\n",
    "# print(one_hot_state)\n",
    "\n",
    "# f = open('dqn_mountaincar.txt', 'w') # file to store the training log\n",
    "temp_file = f\"{runs_folder_path}/w2v_logger_{filename}.txt\"\n",
    "f = open(temp_file, 'w') # file to store the training log\n",
    "reward_curve = [] # this will store the moving avg of rewards\n",
    "moving_window = deque(maxlen=100)\n",
    "epsilon = args.epsilon_start\n",
    "count = 0\n",
    "for e in tqdm(range(args.n_episodes)):\n",
    "    state, _ = env.reset(seed=seed)\n",
    "    curr_reward = 0\n",
    "    for t in range(args.max_esp_len):\n",
    "        action = learner.select_action(state_embeddings[state],epsilon)\n",
    "        # n_state,reward,terminated,truncated,_ = env.step(action)\n",
    "        # done = terminated or truncated\n",
    "        n_state,reward,done,_,_ = env.step(action)\n",
    "        learner.step(state_embeddings[state],action,reward,state_embeddings[n_state],done)\n",
    "        state = n_state\n",
    "        curr_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    moving_window.append(curr_reward)\n",
    "    reward_curve.append(np.mean(moving_window))\n",
    "\n",
    "    \"\"\"\"\n",
    "    TODO: Write code for decaying the exploration rate using args.epsilon_decay\n",
    "    and args.epsilon_end. Note that epsilon has been initialized to args.epsilon_start\n",
    "    1. You are encouraged to try new methods\n",
    "    \"\"\"\n",
    "    ###### TYPE YOUR CODE HERE ######\n",
    "    epsilon *= args.epsilon_decay\n",
    "    epsilon = max(epsilon, args.epsilon_end)\n",
    "    # print('current epsilon = ', epsilon)\n",
    "    #################################\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        print('Episode Number {} Average Episodic Reward (over 100 episodes): {:.2f}'.format(e, np.mean(moving_window)))\n",
    "\n",
    "    f.write('Episode Number {} Average Episodic Reward (over 100 episodes): {:.2f} \\n'.format(e, np.mean(moving_window)))\n",
    "    #################################\n",
    "\n",
    "f.close() # to close the file\n",
    "\n",
    "# Now we save the trained model\n",
    "rew_file = f\"{runs_folder_path}/rewards_w2v_dqn_modified_{modified}_w2v_embed_dim_{embed_dim}_{filename}_epochs_{w2v_epochs}.npy\"\n",
    "np.save(rew_file, reward_curve)\n",
    "model_saved_file = f\"{runs_folder_path}/w2v_dqn_modified_{modified}_w2v_embed_dim_{embed_dim}_{filename}_epochs_{w2v_epochs}.pt\"\n",
    "torch.save(learner.Q.state_dict(), model_saved_file)\n",
    "print(\"Model saved at: \", model_saved_file)\n",
    "\n",
    "final_policy = learner.get_optimal_policy(state_embeddings)\n",
    "print(\"Final policy: \", final_policy)\n",
    "state = 0\n",
    "# Define action map\n",
    "action_map = {\n",
    "0: \"Left\",\n",
    "1: \"Down\",\n",
    "2: \"Right\",\n",
    "3: \"Up\"\n",
    "}\n",
    "print(\"State: Type -    action taken\")\n",
    "lake_grid = env.unwrapped.desc  # Gets the grid representation\n",
    "for row in lake_grid:\n",
    "    for cell in row:\n",
    "        print(f\"     {state}:   {cell.decode('utf-8')} - {final_policy[state]}-->{action_map[final_policy[state]]}\")  # Convert byte to string\n",
    "        state += 1\n",
    "\n",
    "print('It was successful!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_env_details:\n",
    "    # Extract the environment description (grid layout)\n",
    "    lake_grid = env.unwrapped.desc  # Gets the grid representation\n",
    "\n",
    "    # Print state-to-symbol mapping\n",
    "    print(\"Frozen Lake Grid Layout:\")\n",
    "    for row in lake_grid:\n",
    "        print(\" \".join(row.astype(str)))\n",
    "\n",
    "    goal_state = None\n",
    "    rows, cols = lake_grid.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if lake_grid[i, j] == b'G':  # 'G' is stored as a byte-string\n",
    "                goal_state = i * cols + j  # Convert (row, col) to state number\n",
    "                break\n",
    "        # end for j\n",
    "    # end for i\n",
    "    print(f\"Goal State: {goal_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla DQN (without w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "  \"\"\"\n",
    "  Q Network: designed to take state as input and give out Q values of actions as output\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    \"\"\"\n",
    "      state_dim (int): state dimenssion\n",
    "      action_dim (int): action dimenssion\n",
    "    \"\"\"\n",
    "    super(QNetwork, self).__init__()\n",
    "    self.l1 = nn.Linear(state_dim, 64)\n",
    "    self.l2 = nn.Linear(64, 64)\n",
    "    self.l3 = nn.Linear(64, action_dim)\n",
    "\n",
    "  def forward(self, state):\n",
    "    q = F.relu(self.l1(state))\n",
    "    q = F.relu(self.l2(q))\n",
    "    return self.l3(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQNAgent():\n",
    "\n",
    "  def __init__(self,\n",
    "   state_dim,\n",
    "   action_dim,\n",
    "   discount=0.99,\n",
    "   tau=1e-3,\n",
    "   lr=5e-4,\n",
    "   update_freq=4,\n",
    "   max_size=int(1e5),\n",
    "   batch_size=64,\n",
    "   gpu_index=0\n",
    "   ):\n",
    "    \"\"\"\n",
    "      state_size (int): dimension of each state\n",
    "      action_size (int): dimension of each action\n",
    "      discount (float): discount factor\n",
    "      tau (float): used to update q-target\n",
    "      lr (float): learning rate\n",
    "      update_freq (int): update frequency of target network\n",
    "      max_size (int): experience replay buffer size\n",
    "      batch_size (int): training batch size\n",
    "      gpu_index (int): GPU used for training\n",
    "    \"\"\"\n",
    "    self.state_dim = state_dim\n",
    "    self.action_dim = action_dim\n",
    "    self.discount = discount\n",
    "    self.tau = tau\n",
    "    self.lr = lr\n",
    "    self.update_freq = update_freq\n",
    "    self.batch_size = batch_size\n",
    "    self.device = torch.device('cuda', index=gpu_index) if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "    # Setting up the NNs\n",
    "    self.Q = QNetwork(state_dim, action_dim).to(self.device)\n",
    "    self.Q_target = QNetwork(state_dim, action_dim).to(self.device)\n",
    "    self.optimizer = optim.Adam(self.Q.parameters(), lr=self.lr)\n",
    "\n",
    "    # Experience Replay Buffer\n",
    "    self.memory = ExperienceReplay(state_dim,1,max_size,self.batch_size,gpu_index)\n",
    "\n",
    "    self.t_train = 0\n",
    "\n",
    "  def step(self, state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    1. Adds (s,a,r,s') to the experience replay buffer, and updates the networks\n",
    "    2. Learns when the experience replay buffer has enough samples\n",
    "    3. Updates target netowork\n",
    "    \"\"\"\n",
    "    self.memory.add(state, action, reward, next_state, done)\n",
    "    self.t_train += 1\n",
    "\n",
    "    if self.memory.size > self.batch_size:\n",
    "      experiences = self.memory.sample()\n",
    "      self.learn(experiences, self.discount) #To be implemented\n",
    "\n",
    "    if (self.t_train % self.update_freq) == 0:\n",
    "      self.target_update(self.Q, self.Q_target, self.tau) #To be implemented\n",
    "\n",
    "  def select_action(self, state, epsilon):\n",
    "    \"\"\"\n",
    "    TODO: Complete this block to select action using epsilon greedy exploration\n",
    "    strategy\n",
    "    Input: state, epsilon\n",
    "    Return: Action\n",
    "    Return Type: int\n",
    "    \"\"\"\n",
    "    ###### TYPE YOUR CODE HERE ######\n",
    "    # We generate a random number between 0 and 1\n",
    "    rand_num = np.random.random()\n",
    "    state = torch.from_numpy(state).to(self.device)\n",
    "    a_opt = np.argmax(self.Q(state).cpu().detach().numpy())\n",
    "    if rand_num<epsilon:\n",
    "      a_list = [y for y in range(self.action_dim)]\n",
    "      # print('a_list = ', a_list)\n",
    "      a_list.remove(a_opt)\n",
    "      # print('a_list = ', a_list)\n",
    "      at = np.random.choice(np.array(a_list))\n",
    "      return (at)\n",
    "    else:\n",
    "      return(a_opt)\n",
    "    #################################\n",
    "\n",
    "  def learn(self, experiences, discount):\n",
    "    \"\"\"\n",
    "    TODO: Complete this block to update the Q-Network using the target network\n",
    "    1. Compute target using  self.Q_target ( target = r + discount * max_b [Q_target(s,b)] )\n",
    "    2. Compute Q(s,a) using self.Q\n",
    "    3. Compute MSE loss between step 1 and step 2\n",
    "    4. Update your network\n",
    "    Input: experiences consisting of states,actions,rewards,next_states and discount factor\n",
    "    Return: None\n",
    "    \"\"\"\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    ###### TYPE YOUR CODE HERE ######\n",
    "    # Step 1:\n",
    "    target = rewards + discount * torch.max(self.Q_target(next_states), axis = 1, keepdim = True).values * (1 - dones)# change Q_target to Q\n",
    "    # Step 2:\n",
    "    Q_sa = torch.take_along_dim(self.Q(states), actions, dim = 1)\n",
    "    # Step 3:\n",
    "    loss = nn.MSELoss()\n",
    "    mse_loss = loss(target, Q_sa)\n",
    "    # Step 4:\n",
    "    self.optimizer.zero_grad()\n",
    "    mse_loss.backward()\n",
    "    self.optimizer.step()\n",
    "    #################################\n",
    "\n",
    "  def target_update(self, Q, Q_target, tau):\n",
    "    \"\"\"\n",
    "    TODO: Update the target network parameters (param_target) using current Q parameters (param_Q)\n",
    "    Perform the update using tau, this ensures that we do not change the target network drastically\n",
    "    1. param_target = tau * param_Q + (1 - tau) * param_target\n",
    "    Input: Q,Q_target,tau\n",
    "    Return: None\n",
    "    \"\"\"\n",
    "    ###### TYPE YOUR CODE HERE ######\n",
    "    for target_param, param in zip(Q_target.parameters(), Q.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "    #################################\n",
    "\n",
    "  def get_optimal_policy(self, states):\n",
    "    \"\"\"Extracts the optimal policy after training.\"\"\"\n",
    "    policy = []\n",
    "    for state in states:\n",
    "      state = torch.from_numpy(state).to(self.device)\n",
    "      policy.append(np.argmax(self.Q(state).cpu().detach().numpy()))\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run the DQN code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "kwargs = {\n",
    "    \"state_dim\":state_dim,\n",
    "    \"action_dim\":action_dim,\n",
    "    \"discount\":gamma,\n",
    "    \"tau\":args.tau,\n",
    "    \"lr\":args.lr,\n",
    "    \"update_freq\":args.update_freq,\n",
    "    \"max_size\":args.max_size,\n",
    "    \"batch_size\":args.batch_size,\n",
    "    \"gpu_index\":args.gpu_index\n",
    "  }\n",
    "learner = DQNAgent(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_state = np.float32(np.eye(state_dim))\n",
    "print(\"One hot state defined. Now we train dqn\")\n",
    "\n",
    "temp_file = f\"{runs_folder_path}/dqn_logger_{filename}.txt\"\n",
    "f = open(temp_file, 'w') # file to store the training log\n",
    "reward_curve = [] # this will store the moving avg of rewards\n",
    "moving_window = deque(maxlen=100)\n",
    "epsilon = args.epsilon_start\n",
    "count = 0\n",
    "for e in tqdm(range(args.n_episodes)):\n",
    "    state, _ = env.reset(seed=seed)\n",
    "    curr_reward = 0\n",
    "    for t in range(args.max_esp_len):\n",
    "        action = learner.select_action(one_hot_state[state],epsilon)\n",
    "        # n_state,reward,terminated,truncated,_ = env.step(action)\n",
    "        # done = terminated or truncated\n",
    "        n_state,reward,done,_,_ = env.step(action)\n",
    "        learner.step(one_hot_state[state],action,reward,one_hot_state[n_state],done)\n",
    "        state = n_state\n",
    "        curr_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    moving_window.append(curr_reward)\n",
    "    reward_curve.append(np.mean(moving_window))\n",
    "    \n",
    "    epsilon *= args.epsilon_decay\n",
    "    epsilon = max(epsilon, args.epsilon_end)\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        print('Episode Number {} Average Episodic Reward (over 100 episodes): {:.2f}'.format(e, np.mean(moving_window)))\n",
    "\n",
    "    f.write('Episode Number {} Average Episodic Reward (over 100 episodes): {:.2f} \\n'.format(e, np.mean(moving_window)))\n",
    "    #################################\n",
    "\n",
    "f.close() # to close the file\n",
    "\n",
    "# Now we save the trained model\n",
    "rew_file = f\"{runs_folder_path}/rewards_dqn_{filename}.npy\"\n",
    "np.save(rew_file, reward_curve)\n",
    "model_saved_file = f\"{runs_folder_path}/dqn_{filename}.pt\"\n",
    "torch.save(learner.Q.state_dict(), model_saved_file)\n",
    "print(\"Model saved at: \", model_saved_file)\n",
    "\n",
    "final_policy = learner.get_optimal_policy(one_hot_state)\n",
    "print(\"Final policy: \", final_policy)\n",
    "state = 0\n",
    "# Define action map\n",
    "action_map = {\n",
    "0: \"Left\",\n",
    "1: \"Down\",\n",
    "2: \"Right\",\n",
    "3: \"Up\"\n",
    "}\n",
    "print(\"State: Type -    action taken\")\n",
    "lake_grid = env.unwrapped.desc  # Gets the grid representation\n",
    "for row in lake_grid:\n",
    "    for cell in row:\n",
    "        print(f\"     {state}:   {cell.decode('utf-8')} - {final_policy[state]}-->{action_map[final_policy[state]]}\")  # Convert byte to string\n",
    "        state += 1\n",
    "\n",
    "print('It was successful!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_env_details:\n",
    "    # Extract the environment description (grid layout)\n",
    "    lake_grid = env.unwrapped.desc  # Gets the grid representation\n",
    "\n",
    "    # Print state-to-symbol mapping\n",
    "    print(\"Frozen Lake Grid Layout:\")\n",
    "    for row in lake_grid:\n",
    "        print(\" \".join(row.astype(str)))\n",
    "\n",
    "    goal_state = None\n",
    "    rows, cols = lake_grid.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if lake_grid[i, j] == b'G':  # 'G' is stored as a byte-string\n",
    "                goal_state = i * cols + j  # Convert (row, col) to state number\n",
    "                break\n",
    "        # end for j\n",
    "    # end for i\n",
    "    print(f\"Goal State: {goal_state}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "replrn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
