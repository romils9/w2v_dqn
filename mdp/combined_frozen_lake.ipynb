{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "from collections import deque, namedtuple\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "# w2v required\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# DQN required\n",
    "import argparse\n",
    "import numpy as np\n",
    "import logging\n",
    "from matplotlib import animation # will be needed for rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All global variables needed\n",
    "Since some or all of these variables are needed for each cell below, it's difficult to put these inside the main function.\\\n",
    "Perhaps, using some args method might work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate custom map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# Function to generate custom map\n",
    "# ============================================================================================\n",
    "def make_env(env_name, env_dim = 4, seed = 42, stochastic = False):\n",
    "    env = gym.make(env_name, desc=generate_random_map(size=env_dim, seed=seed), \n",
    "                   is_slippery = stochastic, render_mode = 'rgb_array')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "env_name: str\n",
    "env_dim: int --> Dimension of the game: 4x4 or 8x8\n",
    "seed\n",
    "stochastic = boolean --> Whether we use is_slippery = True or False\n",
    " '''\n",
    "env_name = \"FrozenLake-v1\"\n",
    "env_dim = 4\n",
    "stochastic = False\n",
    "seed = 42\n",
    "gamma = 0.99 # discount factor in Q computation\n",
    "alpha = 0.1 # learning rate in the table\n",
    "num_episodes_q_table = 100_000\n",
    "convergence_threshold = 1e-4\n",
    "epsilon_start = 1\n",
    "epsilon_decay_q_table = 0.99995\n",
    "epsilon_end = 0.01\n",
    "check_env_details = True\n",
    "\n",
    "# Creating the environment\n",
    "env = make_env(env_name=env_name, env_dim=env_dim, seed = seed, stochastic=stochastic)\n",
    "state_dim = env.observation_space.n\n",
    "action_dim = env.action_space.n\n",
    "print(\"State space: \", env.observation_space.n)\n",
    "print(\"Action space: \", env.action_space.n)\n",
    "\n",
    "# state related variables\n",
    "num_episodes_trajectories = 10_000\n",
    "num_states = state_dim\n",
    "num_actions = action_dim\n",
    "\n",
    "max_esp_len = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# Q-Learning Agent class\n",
    "# ============================================================================================\n",
    "class QLearningAgent:\n",
    "    \"\"\"Q-learning agent.\"\"\"\n",
    "    def __init__(self, num_states, num_actions, gamma=0.99, epsilon=0.1, alpha=0.1):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = np.zeros((num_states, num_actions))  # Initialize Q-table\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            return random.randint(0, self.num_actions - 1)  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state, :])  # Exploit\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        \"\"\"Q-learning update rule.\"\"\"\n",
    "        best_next_action = np.argmax(self.q_table[next_state, :])  # Greedy action for next state\n",
    "        td_target = reward + self.gamma * self.q_table[next_state, best_next_action]\n",
    "        td_error = td_target - self.q_table[state, action]\n",
    "        self.q_table[state, action] += self.alpha * td_error  # Update Q-table\n",
    "\n",
    "    def get_optimal_policy(self):\n",
    "        \"\"\"Extracts the optimal policy after training.\"\"\"\n",
    "        return np.argmax(self.q_table, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train the model using Q-learning for Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# Function to train the model using Q-learning for Frozen Lake\n",
    "# ============================================================================================\n",
    "def run_tabular_q_frozen(env, agent, num_episodes=10, convergence_threshold=1e-4,\n",
    "                         epsilon_start = 1, epsilon_decay = 0.995, epsilon_end = 0.01, seed=42):\n",
    "    reward_curve = [] # this will store the moving avg of rewards\n",
    "    moving_window = deque(maxlen=100)\n",
    "    epsilon = epsilon_start\n",
    "    prev_q_table = np.copy(agent.q_table)  # Store old Q-table\n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state,_ = env.reset(seed=seed)\n",
    "        # print(f\"\\nIn episode {episode}, After reset initial state = {state} and epsilon = {epsilon}\")\n",
    "        curr_reward = 0\n",
    "        max_eps_len = 100\n",
    "        flag = False\n",
    "\n",
    "        for _ in range(max_eps_len):\n",
    "            action = agent.choose_action(state, epsilon)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.update_q_value(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            curr_reward += reward\n",
    "\n",
    "            # # Compute max Q-value change\n",
    "            # q_change = np.max(np.abs(agent.q_table - prev_q_table))\n",
    "            # prev_q_table = np.copy(agent.q_table)\n",
    "\n",
    "            # # Check Q-value convergence\n",
    "            # if q_change < convergence_threshold:\n",
    "            #     print(f\"Q-values converged at Episode {episode+1} with max Q-change: {q_change}\")\n",
    "            #     flag = True\n",
    "            #     break\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        # end while inside an episode\n",
    "        \n",
    "        # Epsilon decay performed at the end of each episode\n",
    "        epsilon *= epsilon_decay\n",
    "        epsilon = max(epsilon, epsilon_end)\n",
    "\n",
    "        # Appending the smoothened reward\n",
    "        moving_window.append(curr_reward)\n",
    "        reward_curve.append(np.mean(moving_window))\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            print(f\"Tabular Q: Episode {episode}: epsilon = {epsilon}, avg reward = {np.mean(moving_window)}\")\n",
    "        # end if\n",
    "\n",
    "        # if flag:\n",
    "        #     break\n",
    "    # end for num_episode\n",
    "\n",
    "    return agent.q_table, agent.get_optimal_policy(), reward_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Tabular Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = make_env(env_name=env_name, env_dim=env_dim, seed = seed, stochastic=stochastic)\n",
    "\n",
    "# Setting seeds\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "if check_env_details:\n",
    "    # Extract the environment description (grid layout)\n",
    "    lake_grid = env.unwrapped.desc  # Gets the grid representation\n",
    "\n",
    "    # Print state-to-symbol mapping\n",
    "    print(\"Frozen Lake Grid Layout:\")\n",
    "    for row in lake_grid:\n",
    "        print(\" \".join(row.astype(str)))\n",
    "\n",
    "    goal_state = None\n",
    "    rows, cols = lake_grid.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if lake_grid[i, j] == b'G':  # 'G' is stored as a byte-string\n",
    "                goal_state = i * cols + j  # Convert (row, col) to state number\n",
    "                break\n",
    "        # end for j\n",
    "    # end for i\n",
    "    print(f\"Goal State: {goal_state}\")\n",
    "# end if check_env\n",
    "\n",
    "state_dim = env.observation_space.n\n",
    "action_dim = env.action_space.n\n",
    "print(\"State space: \", env.observation_space.n)\n",
    "print(\"Action space: \", env.action_space.n)\n",
    "\n",
    "learner = QLearningAgent(num_states=state_dim, num_actions=action_dim, gamma=gamma\n",
    "                            , epsilon=epsilon_start, alpha=alpha) # Creating the learning Agent\n",
    "\n",
    "final_q_table, final_policy, reward_curve = run_tabular_q_frozen(\n",
    "                env, learner, num_episodes=num_episodes_q_table, convergence_threshold=convergence_threshold,\n",
    "                epsilon_start = epsilon_start, epsilon_decay = epsilon_decay_q_table, epsilon_end = epsilon_end, seed=seed)\n",
    "\n",
    "Val_f = np.max(final_q_table, axis=1)\n",
    "\n",
    "state = 0\n",
    "# Define action map\n",
    "action_map = {\n",
    "0: \"Left\",\n",
    "1: \"Down\",\n",
    "2: \"Right\",\n",
    "3: \"Up\"\n",
    "}\n",
    "print(\"State: Type -    V(s),    action taken\")\n",
    "lake_grid = env.unwrapped.desc  # Gets the grid representation\n",
    "for row in lake_grid:\n",
    "    for cell in row:\n",
    "        print(f\"     {state}:   {cell.decode('utf-8')} - {Val_f[state]:.2f}, {final_policy[state]}-->{action_map[final_policy[state]]}\")  # Convert byte to string\n",
    "        state += 1\n",
    "# assert False, \"c1\"\n",
    "\n",
    "# Print the final table and policy\n",
    "print(\"Final Q function: \", final_q_table)\n",
    "# print(\"Final Policy: \", final_policy)\n",
    "# print(\"Final Value function: \", Val_f)\n",
    "\n",
    "# # Plot heatmap of the Value function\n",
    "# plt.figure(figsize=(5,5))\n",
    "# plt.imshow(Val_f.reshape(4,4), cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "# for i in range(4):\n",
    "#     for j in range(4):\n",
    "#         plt.text(j, i, f\"{Val_f[i*4+j]:.2f}\", ha='center', va='center', color='black')\n",
    "\n",
    "\n",
    "# Plot the reward curve\n",
    "\n",
    "\n",
    "\n",
    "# Save the current Q-function\n",
    "file_name = f\"Q_table_{env_name}_map_size_{env_dim}_stochastic_{stochastic}_seed_{seed}.npy\"\n",
    "np.save(file_name, final_q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Trajectories\n",
    "Here we may need to ensure that the trajectories sufficiently explore each state.\\\n",
    "Thus, we may want to start from specific states when reseting the environment during trajectory collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we need to ensure that the same states are sampled to compare the performance of w2v imbued DQN and Vanilla DQN. \n",
    "Thus we reset np, random and torch.manual_seed before starting both the following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN with w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla DQN (without w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "replrn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
