{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "from collections import deque, namedtuple\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "# w2v required\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# DQN required\n",
    "import argparse\n",
    "import numpy as np\n",
    "import logging\n",
    "from matplotlib import animation # will be needed for rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All global variables needed\n",
    "Since some or all of these variables are needed for each cell below, it's difficult to put these inside the main function.\\\n",
    "Perhaps, using some args method might work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate custom map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# Function to generate custom map\n",
    "# ============================================================================================\n",
    "def make_env(env_name, env_dim = 4, seed = 42, stochastic = False):\n",
    "    env = gym.make(env_name, desc=generate_random_map(size=env_dim, seed=seed), \n",
    "                   is_slippery = stochastic, render_mode = 'rgb_array')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  16\n",
      "Action space:  4\n",
      "Folder 'mdp/runs_frozen' created successfully.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "env_name: str\n",
    "env_dim: int --> Dimension of the game: 4x4 or 8x8\n",
    "seed\n",
    "stochastic = boolean --> Whether we use is_slippery = True or False\n",
    " '''\n",
    "env_name = \"FrozenLake-v1\"\n",
    "env_dim = 4\n",
    "stochastic = False\n",
    "seed = 42\n",
    "gamma = 0.99 # discount factor in Q computation\n",
    "alpha = 0.1 # learning rate in the table\n",
    "num_episodes_q_table = 100_000\n",
    "convergence_threshold = 1e-4\n",
    "epsilon_start = 1\n",
    "epsilon_decay_q_table = 0.99995\n",
    "epsilon_end = 0.01\n",
    "check_env_details = True\n",
    "\n",
    "# Creating the environment\n",
    "env = make_env(env_name=env_name, env_dim=env_dim, seed = seed, stochastic=stochastic)\n",
    "state_dim = env.observation_space.n\n",
    "action_dim = env.action_space.n\n",
    "print(\"State space: \", env.observation_space.n)\n",
    "print(\"Action space: \", env.action_space.n)\n",
    "\n",
    "# state and trajectories related variables\n",
    "num_episodes_trajectories = 10_000\n",
    "num_states = state_dim\n",
    "num_actions = action_dim\n",
    "max_eps_len = 100\n",
    "modified = \"perfect\" # or random - for purely random trajectories or \"False\" for the combined trajs\n",
    "\n",
    "# w2v related variables\n",
    "''' Potential values for embedding dimensions = {4, 8, 12, 16, 20, 32, 64} '''\n",
    "# w2v hyperparameters\n",
    "embed_dim = 32\n",
    "window_size = 2\n",
    "batch_size = 16\n",
    "w2v_epochs = 50\n",
    "w2v_lr = 0.01\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Filename for saving purposes - common part for all files\n",
    "filename = f\"{env_name}_map_size_{env_dim}_stochastic_{stochastic}_seed_{seed}\"\n",
    "\n",
    "# This folder path will be used to store all the saved models and the associated data\n",
    "runs_folder_path = \"mdp/runs_frozen\"\n",
    "\n",
    "# Check if the folder exists\n",
    "if not os.path.exists(runs_folder_path):\n",
    "    # Create the folder\n",
    "    os.makedirs(runs_folder_path)\n",
    "    print(f\"Folder '{runs_folder_path}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{runs_folder_path}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# Q-Learning Agent class\n",
    "# ============================================================================================\n",
    "class QLearningAgent:\n",
    "    \"\"\"Q-learning agent.\"\"\"\n",
    "    def __init__(self, num_states, num_actions, gamma=0.99, epsilon=0.1, alpha=0.1):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = np.zeros((num_states, num_actions))  # Initialize Q-table\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            return random.randint(0, self.num_actions - 1)  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state, :])  # Exploit\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        \"\"\"Q-learning update rule.\"\"\"\n",
    "        best_next_action = np.argmax(self.q_table[next_state, :])  # Greedy action for next state\n",
    "        td_target = reward + self.gamma * self.q_table[next_state, best_next_action]\n",
    "        td_error = td_target - self.q_table[state, action]\n",
    "        self.q_table[state, action] += self.alpha * td_error  # Update Q-table\n",
    "\n",
    "    def get_optimal_policy(self):\n",
    "        \"\"\"Extracts the optimal policy after training.\"\"\"\n",
    "        return np.argmax(self.q_table, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train the model using Q-learning for Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# Function to train the model using Q-learning for Frozen Lake\n",
    "# ============================================================================================\n",
    "def run_tabular_q_frozen(env, agent, num_episodes=10, max_eps_len = 100, convergence_threshold=1e-4,\n",
    "                         epsilon_start = 1, epsilon_decay = 0.995, epsilon_end = 0.01, seed=42):\n",
    "    reward_curve = [] # this will store the moving avg of rewards\n",
    "    moving_window = deque(maxlen=100)\n",
    "    epsilon = epsilon_start\n",
    "    prev_q_table = np.copy(agent.q_table)  # Store old Q-table\n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state,_ = env.reset(seed=seed)\n",
    "        # print(f\"\\nIn episode {episode}, After reset initial state = {state} and epsilon = {epsilon}\")\n",
    "        curr_reward = 0\n",
    "        flag = False\n",
    "\n",
    "        for _ in range(max_eps_len):\n",
    "            action = agent.choose_action(state, epsilon)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.update_q_value(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            curr_reward += reward\n",
    "\n",
    "            # # Compute max Q-value change\n",
    "            # q_change = np.max(np.abs(agent.q_table - prev_q_table))\n",
    "            # prev_q_table = np.copy(agent.q_table)\n",
    "\n",
    "            # # Check Q-value convergence\n",
    "            # if q_change < convergence_threshold:\n",
    "            #     print(f\"Q-values converged at Episode {episode+1} with max Q-change: {q_change}\")\n",
    "            #     flag = True\n",
    "            #     break\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        # end while inside an episode\n",
    "        \n",
    "        # Epsilon decay performed at the end of each episode\n",
    "        epsilon *= epsilon_decay\n",
    "        epsilon = max(epsilon, epsilon_end)\n",
    "\n",
    "        # Appending the smoothened reward\n",
    "        moving_window.append(curr_reward)\n",
    "        reward_curve.append(np.mean(moving_window))\n",
    "\n",
    "        if episode % 1000 == 0:\n",
    "            print(f\"Tabular Q: Episode {episode}: epsilon = {epsilon}, avg reward = {np.mean(moving_window)}\")\n",
    "        # end if\n",
    "\n",
    "        # if flag:\n",
    "        #     break\n",
    "    # end for num_episode\n",
    "\n",
    "    return agent.q_table, agent.get_optimal_policy(), reward_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Tabular Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen Lake Grid Layout:\n",
      "S F H F\n",
      "F H F F\n",
      "F F F H\n",
      "F H F G\n",
      "Goal State: 15\n",
      "State space:  16\n",
      "Action space:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1697/100000 [00:00<00:05, 16968.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 0: epsilon = 0.99995, avg reward = 0.0\n",
      "Tabular Q: Episode 1000: epsilon = 0.9511806740132733, avg reward = 0.0\n",
      "Tabular Q: Episode 2000: epsilon = 0.904789914112052, avg reward = 0.03\n",
      "Tabular Q: Episode 3000: epsilon = 0.8606617134311852, avg reward = 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5071/100000 [00:00<00:05, 16529.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 4000: epsilon = 0.8186857229650423, avg reward = 0.04\n",
      "Tabular Q: Episode 5000: epsilon = 0.7787569756237134, avg reward = 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6725/100000 [00:00<00:05, 16393.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 6000: epsilon = 0.7407756237474893, avg reward = 0.08\n",
      "Tabular Q: Episode 7000: epsilon = 0.7046466894232127, avg reward = 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8365/100000 [00:00<00:05, 16258.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 8000: epsilon = 0.6702798269781429, avg reward = 0.14\n",
      "Tabular Q: Episode 9000: epsilon = 0.6375890970574211, avg reward = 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 9992/100000 [00:00<00:05, 15913.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 10000: epsilon = 0.6064927517201779, avg reward = 0.23\n",
      "Tabular Q: Episode 11000: epsilon = 0.5769130300168653, avg reward = 0.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13186/100000 [00:00<00:05, 15920.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 12000: epsilon = 0.5487759635366593, avg reward = 0.33\n",
      "Tabular Q: Episode 13000: epsilon = 0.5220111914386566, avg reward = 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 14844/100000 [00:00<00:05, 16120.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 14000: epsilon = 0.49655178450431714, avg reward = 0.27\n",
      "Tabular Q: Episode 15000: epsilon = 0.47233407777119996, avg reward = 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 16468/100000 [00:01<00:05, 16154.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 16000: epsilon = 0.44929751132941503, avg reward = 0.32\n",
      "Tabular Q: Episode 17000: epsilon = 0.4273844788827431, avg reward = 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18084/100000 [00:01<00:05, 16142.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 18000: epsilon = 0.40654018369568434, avg reward = 0.39\n",
      "Tabular Q: Episode 19000: epsilon = 0.3867125015662202, avg reward = 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 19707/100000 [00:01<00:04, 16166.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 20000: epsilon = 0.367851850481642, avg reward = 0.54\n",
      "Tabular Q: Episode 21000: epsilon = 0.34991106663148985, avg reward = 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 22976/100000 [00:01<00:04, 16260.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 22000: epsilon = 0.332845286467567, avg reward = 0.51\n",
      "Tabular Q: Episode 23000: epsilon = 0.31661183451608793, avg reward = 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 24603/100000 [00:01<00:04, 16199.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 24000: epsilon = 0.30117011666142574, avg reward = 0.64\n",
      "Tabular Q: Episode 25000: epsilon = 0.286481518634604, avg reward = 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 26258/100000 [00:01<00:04, 16303.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 26000: epsilon = 0.2725093094526817, avg reward = 0.6\n",
      "Tabular Q: Episode 27000: epsilon = 0.259218549567572, avg reward = 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 27915/100000 [00:01<00:04, 16382.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 28000: epsilon = 0.246576003494601, avg reward = 0.61\n",
      "Tabular Q: Episode 29000: epsilon = 0.23455005670232934, avg reward = 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 31288/100000 [00:01<00:04, 16626.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 30000: epsilon = 0.22311063655580088, avg reward = 0.7\n",
      "Tabular Q: Episode 31000: epsilon = 0.21222913711553318, avg reward = 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 32978/100000 [00:02<00:04, 16707.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 32000: epsilon = 0.20187834760418902, avg reward = 0.64\n",
      "Tabular Q: Episode 33000: epsilon = 0.19203238436205666, avg reward = 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 34673/100000 [00:02<00:03, 16777.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 34000: epsilon = 0.18266662612118353, avg reward = 0.71\n",
      "Tabular Q: Episode 35000: epsilon = 0.17375765243629993, avg reward = 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 36407/100000 [00:02<00:03, 16944.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 36000: epsilon = 0.16528318511857973, avg reward = 0.79\n",
      "Tabular Q: Episode 37000: epsilon = 0.15722203252577774, avg reward = 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38116/100000 [00:02<00:03, 16987.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 38000: epsilon = 0.14955403656943475, avg reward = 0.78\n",
      "Tabular Q: Episode 39000: epsilon = 0.14226002230663626, avg reward = 0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 39834/100000 [00:02<00:03, 17044.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 40000: epsilon = 0.1353217499902697, avg reward = 0.87\n",
      "Tabular Q: Episode 41000: epsilon = 0.12872186945787317, avg reward = 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43336/100000 [00:02<00:03, 17280.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 42000: epsilon = 0.12244387674502544, avg reward = 0.82\n",
      "Tabular Q: Episode 43000: epsilon = 0.11647207281477254, avg reward = 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45065/100000 [00:02<00:03, 17178.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 44000: epsilon = 0.11079152429989349, avg reward = 0.86\n",
      "Tabular Q: Episode 45000: epsilon = 0.10538802615983875, avg reward = 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 46803/100000 [00:02<00:03, 17237.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 46000: epsilon = 0.10024806615895202, avg reward = 0.8\n",
      "Tabular Q: Episode 47000: epsilon = 0.09535879107715316, avg reward = 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 48550/100000 [00:02<00:02, 17304.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 48000: epsilon = 0.09070797456858609, avg reward = 0.89\n",
      "Tabular Q: Episode 49000: epsilon = 0.08628398658785626, avg reward = 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50288/100000 [00:03<00:02, 17324.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 50000: epsilon = 0.08207576430740496, avg reward = 0.89\n",
      "Tabular Q: Episode 51000: epsilon = 0.07807278445329505, avg reward = 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52051/100000 [00:03<00:02, 17413.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 52000: epsilon = 0.07426503699022786, avg reward = 0.89\n",
      "Tabular Q: Episode 53000: epsilon = 0.07064300008999028, avg reward = 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 53832/100000 [00:03<00:02, 17530.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 54000: epsilon = 0.06719761632073298, avg reward = 0.91\n",
      "Tabular Q: Episode 55000: epsilon = 0.06392026999754027, avg reward = 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57371/100000 [00:03<00:02, 17605.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 56000: epsilon = 0.06080276563765279, avg reward = 0.88\n",
      "Tabular Q: Episode 57000: epsilon = 0.05783730746646704, avg reward = 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59132/100000 [00:03<00:02, 17566.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 58000: epsilon = 0.05501647992306336, avg reward = 0.93\n",
      "Tabular Q: Episode 59000: epsilon = 0.05233322911651293, avg reward = 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 60895/100000 [00:03<00:02, 17583.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 60000: epsilon = 0.04978084518659529, avg reward = 0.91\n",
      "Tabular Q: Episode 61000: epsilon = 0.04735294552481254, avg reward = 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 62654/100000 [00:03<00:02, 17448.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 62000: epsilon = 0.0450434588137458, avg reward = 0.92\n",
      "Tabular Q: Episode 63000: epsilon = 0.04284660984484019, avg reward = 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64400/100000 [00:03<00:02, 17386.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 64000: epsilon = 0.04075690507665269, avg reward = 0.97\n",
      "Tabular Q: Episode 65000: epsilon = 0.03876911889745041, avg reward = 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66182/100000 [00:03<00:01, 17514.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 66000: epsilon = 0.03687828055780553, avg reward = 0.98\n",
      "Tabular Q: Episode 67000: epsilon = 0.03507966174051114, avg reward = 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 67972/100000 [00:04<00:01, 17626.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 68000: epsilon = 0.03336876473673393, avg reward = 0.98\n",
      "Tabular Q: Episode 69000: epsilon = 0.031741311198836865, avg reward = 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 71529/100000 [00:04<00:01, 17718.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 70000: epsilon = 0.03019323144174688, avg reward = 0.93\n",
      "Tabular Q: Episode 71000: epsilon = 0.0287206542661129, avg reward = 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73301/100000 [00:04<00:01, 17718.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 72000: epsilon = 0.027319897277807384, avg reward = 0.96\n",
      "Tabular Q: Episode 73000: epsilon = 0.025987457679562245, avg reward = 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75073/100000 [00:04<00:01, 17689.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 74000: epsilon = 0.02472000351171302, avg reward = 0.95\n",
      "Tabular Q: Episode 75000: epsilon = 0.0235143653201477, avg reward = 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 76856/100000 [00:04<00:01, 17729.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 76000: epsilon = 0.02236752823062398, avg reward = 0.95\n",
      "Tabular Q: Episode 77000: epsilon = 0.021276624409636378, avg reward = 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 78629/100000 [00:04<00:01, 17628.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 78000: epsilon = 0.0202389258929799, avg reward = 0.97\n",
      "Tabular Q: Episode 79000: epsilon = 0.019251837764077552, avg reward = 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80408/100000 [00:04<00:01, 17674.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 80000: epsilon = 0.018312891665012748, avg reward = 0.99\n",
      "Tabular Q: Episode 81000: epsilon = 0.017419739624040163, avg reward = 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82181/100000 [00:04<00:01, 17689.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 82000: epsilon = 0.016570148184139464, avg reward = 0.99\n",
      "Tabular Q: Episode 83000: epsilon = 0.015761992817930556, avg reward = 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 83977/100000 [00:04<00:00, 17767.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 84000: epsilon = 0.014993252614982331, avg reward = 1.0\n",
      "Tabular Q: Episode 85000: epsilon = 0.014262005228231584, avg reward = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 87550/100000 [00:05<00:00, 17819.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 86000: epsilon = 0.01356642206687351, avg reward = 0.98\n",
      "Tabular Q: Episode 87000: epsilon = 0.012904763723703517, avg reward = 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89339/100000 [00:05<00:00, 17838.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 88000: epsilon = 0.012275375625475645, avg reward = 0.98\n",
      "Tabular Q: Episode 89000: epsilon = 0.011676683895400804, avg reward = 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91123/100000 [00:05<00:00, 17813.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 90000: epsilon = 0.011107191417438142, avg reward = 0.99\n",
      "Tabular Q: Episode 91000: epsilon = 0.010565474092537885, avg reward = 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 92905/100000 [00:05<00:00, 17803.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 92000: epsilon = 0.010050177277473855, avg reward = 0.99\n",
      "Tabular Q: Episode 93000: epsilon = 0.01, avg reward = 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 94686/100000 [00:05<00:00, 17594.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 94000: epsilon = 0.01, avg reward = 0.98\n",
      "Tabular Q: Episode 95000: epsilon = 0.01, avg reward = 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 96447/100000 [00:05<00:00, 17533.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 96000: epsilon = 0.01, avg reward = 0.98\n",
      "Tabular Q: Episode 97000: epsilon = 0.01, avg reward = 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98265/100000 [00:05<00:00, 17723.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular Q: Episode 98000: epsilon = 0.01, avg reward = 0.97\n",
      "Tabular Q: Episode 99000: epsilon = 0.01, avg reward = 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:05<00:00, 17153.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: Type -    V(s),    action taken\n",
      "     0:   S - 0.95, 1-->Down\n",
      "     1:   F - 0.94, 0-->Left\n",
      "     2:   H - 0.00, 0-->Left\n",
      "     3:   F - 0.93, 1-->Down\n",
      "     4:   F - 0.96, 1-->Down\n",
      "     5:   H - 0.00, 0-->Left\n",
      "     6:   F - 0.98, 1-->Down\n",
      "     7:   F - 0.97, 0-->Left\n",
      "     8:   F - 0.97, 2-->Right\n",
      "     9:   F - 0.98, 2-->Right\n",
      "     10:   F - 0.99, 1-->Down\n",
      "     11:   H - 0.00, 0-->Left\n",
      "     12:   F - 0.96, 3-->Up\n",
      "     13:   H - 0.00, 0-->Left\n",
      "     14:   F - 1.00, 2-->Right\n",
      "     15:   G - 0.00, 0-->Left\n",
      "Final Q function:  [[0.94148015 0.95099005 0.93206535 0.94148015]\n",
      " [0.94148015 0.         0.         0.93206535]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.92984196 0.49090644 0.23199305]\n",
      " [0.95099005 0.96059601 0.         0.94148015]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9801     0.96059601 0.        ]\n",
      " [0.970299   0.         0.94446586 0.82700786]\n",
      " [0.96059601 0.95099005 0.970299   0.95099005]\n",
      " [0.96059601 0.         0.9801     0.        ]\n",
      " [0.970299   0.99       0.         0.970299  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.95099005 0.95099005 0.         0.96059601]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.99       1.         0.9801    ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# env = make_env(env_name=env_name, env_dim=env_dim, seed = seed, stochastic=stochastic)\n",
    "\n",
    "# Setting seeds\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "if check_env_details:\n",
    "    # Extract the environment description (grid layout)\n",
    "    lake_grid = env.unwrapped.desc  # Gets the grid representation\n",
    "\n",
    "    # Print state-to-symbol mapping\n",
    "    print(\"Frozen Lake Grid Layout:\")\n",
    "    for row in lake_grid:\n",
    "        print(\" \".join(row.astype(str)))\n",
    "\n",
    "    goal_state = None\n",
    "    rows, cols = lake_grid.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if lake_grid[i, j] == b'G':  # 'G' is stored as a byte-string\n",
    "                goal_state = i * cols + j  # Convert (row, col) to state number\n",
    "                break\n",
    "        # end for j\n",
    "    # end for i\n",
    "    print(f\"Goal State: {goal_state}\")\n",
    "# end if check_env\n",
    "\n",
    "state_dim = env.observation_space.n\n",
    "action_dim = env.action_space.n\n",
    "print(\"State space: \", env.observation_space.n)\n",
    "print(\"Action space: \", env.action_space.n)\n",
    "\n",
    "learner = QLearningAgent(num_states=state_dim, num_actions=action_dim, gamma=gamma\n",
    "                            , epsilon=epsilon_start, alpha=alpha) # Creating the learning Agent\n",
    "\n",
    "final_q_table, final_policy, reward_curve = run_tabular_q_frozen(\n",
    "                env, learner, num_episodes=num_episodes_q_table, max_eps_len=max_eps_len, convergence_threshold=convergence_threshold,\n",
    "                epsilon_start = epsilon_start, epsilon_decay = epsilon_decay_q_table, epsilon_end = epsilon_end, seed=seed)\n",
    "\n",
    "Val_f = np.max(final_q_table, axis=1)\n",
    "\n",
    "state = 0\n",
    "# Define action map\n",
    "action_map = {\n",
    "0: \"Left\",\n",
    "1: \"Down\",\n",
    "2: \"Right\",\n",
    "3: \"Up\"\n",
    "}\n",
    "print(\"State: Type -    V(s),    action taken\")\n",
    "lake_grid = env.unwrapped.desc  # Gets the grid representation\n",
    "for row in lake_grid:\n",
    "    for cell in row:\n",
    "        print(f\"     {state}:   {cell.decode('utf-8')} - {Val_f[state]:.2f}, {final_policy[state]}-->{action_map[final_policy[state]]}\")  # Convert byte to string\n",
    "        state += 1\n",
    "# assert False, \"c1\"\n",
    "\n",
    "# Print the final table and policy\n",
    "print(\"Final Q function: \", final_q_table)\n",
    "# print(\"Final Policy: \", final_policy)\n",
    "# print(\"Final Value function: \", Val_f)\n",
    "\n",
    "# # Plot heatmap of the Value function\n",
    "# plt.figure(figsize=(5,5))\n",
    "# plt.imshow(Val_f.reshape(4,4), cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "# for i in range(4):\n",
    "#     for j in range(4):\n",
    "#         plt.text(j, i, f\"{Val_f[i*4+j]:.2f}\", ha='center', va='center', color='black')\n",
    "\n",
    "\n",
    "# Plot the reward curve\n",
    "\n",
    "\n",
    "\n",
    "# Save the current Q-function\n",
    "save_model = f\"{runs_folder_path}/Q_table_{filename}.npy\"\n",
    "np.save(save_model, final_q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Trajectories\n",
    "Here we may need to ensure that the trajectories sufficiently explore each state.\\\n",
    "Thus, we may want to start from specific states when reseting the environment during trajectory collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(q_table, state, epsilon):\n",
    "    \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, num_actions - 1)  # Explore\n",
    "    else:\n",
    "        return np.argmax(q_table[state, :])  # Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 18772.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectories Saved in mdp/runs_frozen/modified_perfect_trajectories_FrozenLake-v1_map_size_4_stochastic_False_seed_42.npy!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "q_table = np.load(f\"{runs_folder_path}/Q_table_{filename}.npy\")\n",
    "# env = make_env(env_name=env_name, env_dim=env_dim, seed = seed, stochastic=stochastic)\n",
    "\n",
    "traj = []\n",
    "tuple_traj = False\n",
    "for e in tqdm(range(num_episodes_trajectories)):\n",
    "    # print(e)\n",
    "    if modified==\"perfect\":\n",
    "        _, _ = env.reset()\n",
    "        epsilon = 0.01\n",
    "        state = int(e%16)\n",
    "    # end if perfect\n",
    "\n",
    "    elif modified==\"random\":\n",
    "        epsilon = 1\n",
    "        if random.uniform(0, 1) < 0.1:\n",
    "            if random.uniform(0, 1) < 0.5:\n",
    "                state = 3\n",
    "            elif random.uniform(0, 1) < 0.65:\n",
    "                state = 6\n",
    "            else:\n",
    "                state = 7\n",
    "            # end if choosing state\n",
    "        else:\n",
    "            state, _ = env.reset(seed=seed)\n",
    "    # end if random trajectories\n",
    "    \n",
    "    else:\n",
    "        # continue    \n",
    "        if e<0.1*num_episodes_trajectories:\n",
    "          epsilon = 1\n",
    "        elif e<0.2*num_episodes_trajectories:\n",
    "          epsilon = 0.8\n",
    "        elif e<0.3*num_episodes_trajectories:\n",
    "          epsilon = 0.6\n",
    "        elif e<0.4*num_episodes_trajectories:\n",
    "          epsilon = 0.4\n",
    "        elif e<0.5*num_episodes_trajectories:\n",
    "          epsilon = 0.2\n",
    "        elif e<0.6*num_episodes_trajectories:\n",
    "          epsilon = 0.1\n",
    "        elif e<0.7*num_episodes_trajectories:\n",
    "          epsilon = 0.05\n",
    "        else:\n",
    "          epsilon = 0.01\n",
    "        # end epsilon if\n",
    "        if random.uniform(0, 1) < 0.1:\n",
    "            if random.uniform(0, 1) < 0.5:\n",
    "                state = 3\n",
    "            elif random.uniform(0, 1) < 0.65:\n",
    "                state = 6\n",
    "            else:\n",
    "                state = 7\n",
    "            # end if choosing state\n",
    "        else:\n",
    "            state, _ = env.reset(seed=seed)\n",
    "    \n",
    "    if not tuple_traj:\n",
    "       traj.append('s_'+str(state))\n",
    "    '''\n",
    "    Action Stochasticity (is_slippery=True): The seed affects how the agent slips \n",
    "    (randomly moves instead of following the chosen action).\n",
    "    Random Hole Placement (if map is generated dynamically): If the map has random \n",
    "    elements, different seeds can affect the placement of H (holes).\n",
    "    '''\n",
    "    curr_reward = 0\n",
    "    rep_count = 0\n",
    "    for t in range(max_eps_len):\n",
    "      action = choose_action(q_table, state, epsilon) #To be implemented\n",
    "      n_state,reward,done,_,_ = env.step(action)\n",
    "      \n",
    "      # We store the current tuple\n",
    "      if tuple_traj:\n",
    "        traj.append((state, action, reward, n_state, done))\n",
    "      else:\n",
    "        temp = 's_'+str(n_state)\n",
    "        # traj.append('s_'+str(n_state))\n",
    "        traj.append(temp)\n",
    "\n",
    "      state = n_state\n",
    "      curr_reward += reward\n",
    "      if done:\n",
    "        if rep_count>=5: # This forces repetitions to occur when done becomes True thereby repeating ending states\n",
    "            if not tuple_traj:\n",
    "                if temp=='s_15':\n",
    "                    traj.append('s_'+str(16))\n",
    "                else:\n",
    "                    traj.append('s_'+str(17))\n",
    "            break\n",
    "        else:\n",
    "            if not tuple_traj:\n",
    "                traj.append(temp)\n",
    "        rep_count+=1\n",
    "    # end for\n",
    "# end for\n",
    "if tuple_traj:\n",
    "    save_file_name = f\"{runs_folder_path}/modified_tuple_trajectories_{filename}.npy\"\n",
    "    np.save(save_file_name, traj)\n",
    "else:\n",
    "    save_file_name = f\"{runs_folder_path}/modified_{modified}_trajectories_{filename}.npy\"\n",
    "    np.save(save_file_name, traj)\n",
    "print(f\"Trajectories Saved in {save_file_name}!\")\n",
    "\n",
    "# assert False, \"No w2v business here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trajectories being used are:  mdp/runs_frozen/modified_perfect_trajectories_FrozenLake-v1_map_size_4_stochastic_False_seed_42.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7e621dccf010>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_file = f\"{runs_folder_path}/modified_{modified}_trajectories_{filename}.npy\"\n",
    "print(\"The trajectories being used are: \", traj_file)\n",
    "\n",
    "text = np.load(traj_file)\n",
    "np.random.seed(seed=seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we define the functions for word2vec algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text): # Again we already have a vocabulary hence don't need to use this function directly\n",
    "    word_counts = Counter(text)\n",
    "    vocab = {word: i for i, word in enumerate(word_counts.keys())}\n",
    "    reverse_vocab = {i: word for word, i in vocab.items()}\n",
    "    return vocab, reverse_vocab, word_counts\n",
    "\n",
    "def generate_skipgram_pairs(text, window_size=2): # This function gives the word and context pairs.\n",
    "    # words = tokenize_text(text)\n",
    "    words = text\n",
    "    pairs = []\n",
    "    for i, target_word in enumerate(words):\n",
    "        window_start = max(i - window_size, 0)\n",
    "        window_end = min(i + window_size + 1, len(words))\n",
    "        for j in range(window_start, window_end):\n",
    "            if i != j:\n",
    "                pairs.append((words[i], words[j]))\n",
    "    return pairs\n",
    "\n",
    "# vocab,_,wcounts = build_vocab(text)\n",
    "# print(\"vocab: \", vocab)\n",
    "# print(\"word counts: \", wcounts)\n",
    "# assert False, \"Checking the word counts!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# Classes: Word2vec Dataset creator and SkipGram model\n",
    "# ============================================================================================\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, text, vocab, window_size=2):\n",
    "        self.vocab = vocab\n",
    "        self.data = generate_skipgram_pairs(text, window_size)\n",
    "        self.vocab_size = len(vocab) # why is this needed here?\n",
    "\n",
    "    def __len__(self): # what does this function do?\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target, context = self.data[idx]\n",
    "        ''' Doesn't the above idx reflect the idx numbered pair instead in pairs instead of the pairs\n",
    "        corresponding to the word at idx? '''\n",
    "        target_idx = torch.tensor(self.vocab[target], dtype=torch.long)\n",
    "        context_idx = torch.tensor(self.vocab[context], dtype=torch.long)\n",
    "        # target_idx = self.vocab[target]\n",
    "        # context_idx = self.vocab[context]\n",
    "\n",
    "        return target_idx, context_idx\n",
    "    \n",
    "\n",
    "    # ============================================================================================\n",
    "# Class: SkipGram using softmax over entire vocabulary\n",
    "# ============================================================================================\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Input word embedding\n",
    "        self.in_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Output word embedding (used for context words)\n",
    "        self.out_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Initialize weights (better stability)\n",
    "        init_range = 0.5 / embedding_dim\n",
    "        self.in_embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.out_embedding.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def forward(self, center_word_idx):\n",
    "        \"\"\"Compute word embeddings and softmax probabilities for the context words.\"\"\"\n",
    "        center_embed = F.relu(self.in_embedding(center_word_idx))  # Shape: (batch_size, embedding_dim)\n",
    "        scores = torch.matmul(center_embed, self.out_embedding.weight.T)  # Compute dot product\n",
    "        y_pred = torch.softmax(scores, dim=1)  # Apply softmax over output vocab\n",
    "        return y_pred\n",
    "\n",
    "    def get_word_vector(self, word_idx):\n",
    "        \"\"\"Return the learned embedding vector for a given word index.\"\"\"\n",
    "        return self.in_embedding(word_idx).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================================\n",
    "# Function: to train the w2v skipgram model\n",
    "# ============================================================================================\n",
    "\n",
    "def train_skipgram(model, data_loader, epochs=6, lr=0.01, device = device):\n",
    "    \"\"\"Train the SkipGram model using Adam optimizer.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()  # Cross-entropy for multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_loss = 0\n",
    "        for center_word_idx, context_word_idx in data_loader:\n",
    "            center_word_idx = center_word_idx.to(device)\n",
    "            context_word_idx = context_word_idx.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(center_word_idx)  # Forward pass\n",
    "            loss = criterion(y_pred, context_word_idx)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function for w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:15<12:35, 15.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 110887.2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:32<12:58, 16.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 110893.1169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:49<12:55, 16.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 110992.9873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [01:03<12:09, 15.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 111317.7144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [01:19<11:49, 15.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 111958.4771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [01:34<11:20, 15.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 111973.5527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [01:49<10:57, 15.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 112233.7298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [02:04<10:37, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 112483.0632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [02:19<10:20, 15.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 112483.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [02:34<10:06, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 112481.9490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [02:49<09:48, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 112482.9863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [03:04<09:30, 15.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 112483.3948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [03:19<09:15, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 112483.3878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [03:34<08:58, 14.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 112482.7575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [03:49<08:46, 15.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 112481.4751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [04:06<08:51, 15.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 112483.2553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [04:21<08:31, 15.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 112482.7682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [04:36<08:09, 15.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 112482.8302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [04:51<07:53, 15.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 112482.7034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [05:06<07:38, 15.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 112482.3457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [05:23<07:38, 15.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 112481.9342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [05:40<07:32, 16.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 112482.3104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [05:56<07:08, 15.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: 112797.7654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [06:11<06:46, 15.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss: 112939.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [06:27<06:38, 15.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss: 112940.4721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [06:44<06:24, 16.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss: 112940.2037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [07:00<06:13, 16.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss: 112940.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [07:17<06:01, 16.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss: 112939.3943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [07:34<05:44, 16.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Loss: 113239.2079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [07:48<05:19, 15.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 113417.2709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [08:03<04:57, 15.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Loss: 113417.2871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [08:20<04:44, 15.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Loss: 113417.2807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [08:37<04:36, 16.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Loss: 113417.2742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [08:54<04:24, 16.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Loss: 113417.2474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [09:11<04:08, 16.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Loss: 113417.2636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [09:27<03:51, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Loss: 113417.2547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [09:43<03:33, 16.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Loss: 113417.2798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [09:59<03:14, 16.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Loss: 113417.2628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [10:14<02:54, 15.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Loss: 113417.2969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [10:29<02:36, 15.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Loss: 113417.2790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [10:45<02:21, 15.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Loss: 113417.2806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [11:00<02:03, 15.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Loss: 113417.2644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [11:15<01:46, 15.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Loss: 113417.2782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [11:30<01:32, 15.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Loss: 113417.2969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [11:45<01:16, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Loss: 113417.2945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [12:00<01:00, 15.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Loss: 113417.2807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [12:15<00:45, 15.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Loss: 113417.2457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [12:30<00:30, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Loss: 113417.2645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [12:45<00:14, 14.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Loss: 113417.2709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [13:00<00:00, 15.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 113417.2790\n",
      "W2v vetors stored in:  mdp/runs_frozen/modified_perfect_w2v_embed_dim_32_FrozenLake-v1_map_size_4_stochastic_False_seed_42_epochs_50.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================================\n",
    "#  Main function for calling the w2v agent\n",
    "# ============================================================================================\n",
    "# First we create the dataset and dataloader\n",
    "vocab, _, _ = build_vocab(text)\n",
    "dataset = Word2VecDataset(text, vocab, window_size)\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "model = SkipGram(vocab_size=len(vocab), embedding_dim=embed_dim).to(device)\n",
    "model = train_skipgram(model, dataloader, epochs = w2v_epochs, lr = w2v_lr)\n",
    "\n",
    "# ============================================================================================\n",
    "#  Saving the w2v generated vector embeddings as a dictionary\n",
    "# ============================================================================================\n",
    "word_embeddings = {}\n",
    "for word in vocab:\n",
    "    word_idx = torch.tensor([vocab[word]], dtype=torch.long).to(device)\n",
    "    updated_embedding = model.get_word_vector(word_idx)\n",
    "    # print(f\"Updated embedding for '{word}': {updated_embedding}\")\n",
    "    # Store the embedding in the dictionary\n",
    "    word_embeddings[word] = updated_embedding.flatten()  # Flatten to 1D array\n",
    "\n",
    "# Saving the w2v model\n",
    "save_w2v_file = f\"{runs_folder_path}/modified_{modified}_w2v_embed_dim_{embed_dim}_{filename}_epochs_{w2v_epochs}.npy\"\n",
    "np.save(save_w2v_file, word_embeddings)\n",
    "print(\"W2v vetors stored in: \", save_w2v_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words similar to 's_0':\n",
      "s_7: 0.8252\n",
      "s_9: 0.5571\n",
      "s_6: 0.5460\n",
      "s_8: 0.5050\n",
      "s_12: 0.4732\n",
      "Top 5 words similar to 's_4':\n",
      "s_10: 0.5551\n",
      "s_1: 0.4990\n",
      "s_0: 0.4210\n",
      "s_7: 0.4177\n",
      "s_5: 0.4088\n",
      "Top 5 words similar to 's_8':\n",
      "s_11: 0.6781\n",
      "s_10: 0.5752\n",
      "s_5: 0.5568\n",
      "s_3: 0.5273\n",
      "s_0: 0.5050\n",
      "Top 5 words similar to 's_9':\n",
      "s_0: 0.5571\n",
      "s_5: 0.5321\n",
      "s_7: 0.4793\n",
      "s_6: 0.4463\n",
      "s_1: 0.4430\n",
      "Top 5 words similar to 's_10':\n",
      "s_8: 0.5752\n",
      "s_5: 0.5638\n",
      "s_4: 0.5551\n",
      "s_2: 0.5457\n",
      "s_3: 0.5308\n",
      "Top 5 words similar to 's_14':\n",
      "s_16: 0.3096\n",
      "s_15: 0.0496\n",
      "s_13: -0.0069\n",
      "s_17: -0.0252\n",
      "s_9: -0.0977\n",
      "Top 5 words similar to 's_15':\n",
      "s_16: 0.3565\n",
      "s_14: 0.0496\n",
      "s_13: -0.0071\n",
      "s_17: -0.0441\n",
      "s_8: -0.0796\n",
      "Top 5 words similar to 's_16':\n",
      "s_15: 0.3565\n",
      "s_14: 0.3096\n",
      "s_17: 0.2524\n",
      "s_13: -0.0040\n",
      "s_9: -0.1480\n",
      "Top 5 words similar to 's_1':\n",
      "s_5: 0.5043\n",
      "s_4: 0.4990\n",
      "s_2: 0.4935\n",
      "s_9: 0.4430\n",
      "s_12: 0.4274\n",
      "Top 5 words similar to 's_11':\n",
      "s_8: 0.6781\n",
      "s_2: 0.6654\n",
      "s_3: 0.6033\n",
      "s_5: 0.5503\n",
      "s_7: 0.5113\n",
      "Top 5 words similar to 's_17':\n",
      "s_16: 0.2524\n",
      "s_6: 0.0410\n",
      "s_8: 0.0119\n",
      "s_9: 0.0097\n",
      "s_3: 0.0090\n",
      "Top 5 words similar to 's_2':\n",
      "s_11: 0.6654\n",
      "s_5: 0.6307\n",
      "s_3: 0.6199\n",
      "s_7: 0.6007\n",
      "s_10: 0.5457\n",
      "Top 5 words similar to 's_3':\n",
      "s_2: 0.6199\n",
      "s_11: 0.6033\n",
      "s_5: 0.5642\n",
      "s_10: 0.5308\n",
      "s_8: 0.5273\n",
      "Top 5 words similar to 's_5':\n",
      "s_2: 0.6307\n",
      "s_3: 0.5642\n",
      "s_10: 0.5638\n",
      "s_8: 0.5568\n",
      "s_7: 0.5514\n",
      "Top 5 words similar to 's_6':\n",
      "s_7: 0.6495\n",
      "s_0: 0.5460\n",
      "s_2: 0.4542\n",
      "s_9: 0.4463\n",
      "s_12: 0.4322\n",
      "Top 5 words similar to 's_7':\n",
      "s_0: 0.8252\n",
      "s_6: 0.6495\n",
      "s_2: 0.6007\n",
      "s_5: 0.5514\n",
      "s_10: 0.5214\n",
      "Top 5 words similar to 's_12':\n",
      "s_10: 0.4929\n",
      "s_0: 0.4732\n",
      "s_2: 0.4667\n",
      "s_3: 0.4340\n",
      "s_6: 0.4322\n",
      "Top 5 words similar to 's_13':\n",
      "s_10: 0.0057\n",
      "s_2: -0.0003\n",
      "s_17: -0.0024\n",
      "s_16: -0.0040\n",
      "s_14: -0.0069\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================================\n",
    "# Similarity checking and visualizing \n",
    "# ============================================================================================\n",
    "# import numpy as np\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load saved embeddings\n",
    "word_embeddings = np.load(f\"{runs_folder_path}/modified_{modified}_w2v_embed_dim_{embed_dim}_{filename}_epochs_{w2v_epochs}.npy\", \n",
    "                          allow_pickle=True).item()\n",
    "\n",
    "# Convert to a NumPy array for fast computation\n",
    "words = list(word_embeddings.keys())\n",
    "vectors = np.array(list(word_embeddings.values()))\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(vectors)\n",
    "\n",
    "# Function to find top-N similar words\n",
    "def find_similar_words(target_word, top_n=5):\n",
    "    if target_word not in word_embeddings:\n",
    "        print(f\"Word '{target_word}' not found in vocabulary.\")\n",
    "        return []\n",
    "\n",
    "    # Get index of target word\n",
    "    target_idx = words.index(target_word)\n",
    "\n",
    "    # Get similarity scores for the target word\n",
    "    similarity_scores = cosine_sim_matrix[target_idx]\n",
    "\n",
    "    # Get top-N most similar words (excluding itself)\n",
    "    similar_indices = np.argsort(similarity_scores)[::-1][1:top_n+1]  # Sort in descending order\n",
    "\n",
    "    # Return words with their similarity scores\n",
    "    return [(words[i], similarity_scores[i]) for i in similar_indices]\n",
    "\n",
    "# Example usage\n",
    "for target_word in word_embeddings:\n",
    "    top_similar_words = find_similar_words(target_word, top_n=5)\n",
    "\n",
    "    print(f\"Top 5 words similar to '{target_word}':\")\n",
    "    for word, score in top_similar_words:\n",
    "        print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we need to ensure that the same states are sampled to compare the performance of w2v imbued DQN and Vanilla DQN. \n",
    "Thus we reset np, random and torch.manual_seed before starting both the following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN common set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining all classes\n",
    "We define the following classes:\n",
    "1. Experience Replay\n",
    "2. DQNAgent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla DQN (without w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "replrn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
