{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECNpbqa8sJbo"
      },
      "source": [
        "# w2v based representation learning\n",
        "We wish to perform w2v on a randomly generated MDP.\n",
        "\n",
        "1.  Initialize |S| and |A|.\n",
        "2.  Generate a Transition Probability Matrix ð™¿. Note: keep a seed fixed to ensure reproducibility.\n",
        "3. Randomly initialize a reward vector associated with each (s,a) ensuring that r(s,a) âˆˆ [0,1].\n",
        "4.  Finally generate the trajectory and save it as a .npy file. Load this file into the dataloader of the w2v_model class\n",
        "5. Perform w2v\n",
        "6. Ensure that the initial vector embedding associated with each state as obtained from the weights of the initial layer are saved since we will perform comparison with these initial weights and the ones obtained from w2v. \\\\\n",
        "To understand how the layers are trained, check the Q-learning code for any MDP game such as frozen lake etc. This will clarify the Q-learning set-up and how we would go about solving the problem using Q-learning. \\\\\n",
        "### Finally, we assume minimality of the MDP i.e. p_{i,j}^t>0 for some t>0 âˆ€ i,j âˆˆ [|S|]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "novJdTyNvIn-"
      },
      "source": [
        "### We import all the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEfo5OBCsDza"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpIsh2QoUL_W",
        "outputId": "d2984d0a-7241-4f7c-e9c6-92c4a30da40f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# # connect gdrive here\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPpwt5pmvM7Q"
      },
      "source": [
        "### We compute the MDP parameters and define the list storing the names of each (s,a) pair i.e. each word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzRhDKv_sHIf",
        "outputId": "cef2231d-3377-4490-ddd4-d01c8e1e8709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of word_space i.e. |S|x|A| =  16\n",
            "List of words:  ['s0_0', 's0_1', 's0_2', 's0_3', 's1_0', 's1_1', 's1_2', 's1_3', 's2_0', 's2_1', 's2_2', 's2_3', 's3_0', 's3_1', 's3_2', 's3_3']\n",
            "Final Probability matrix:  [[[0.26923077 0.15384615 0.23076923 0.34615385]\n",
            "  [0.10526316 0.31578947 0.36842105 0.21052632]\n",
            "  [0.15789474 0.36842105 0.36842105 0.10526316]\n",
            "  [0.29411765 0.23529412 0.05882353 0.41176471]]\n",
            "\n",
            " [[0.5        0.1        0.4        0.        ]\n",
            "  [0.40909091 0.22727273 0.36363636 0.        ]\n",
            "  [0.45       0.1        0.3        0.15      ]\n",
            "  [0.5        0.125      0.25       0.125     ]]\n",
            "\n",
            " [[0.25       0.16666667 0.33333333 0.25      ]\n",
            "  [0.07692308 0.23076923 0.61538462 0.07692308]\n",
            "  [0.3        0.26666667 0.3        0.13333333]\n",
            "  [0.05882353 0.17647059 0.35294118 0.41176471]]\n",
            "\n",
            " [[0.33333333 0.         0.5        0.16666667]\n",
            "  [0.4375     0.1875     0.0625     0.3125    ]\n",
            "  [0.22727273 0.40909091 0.13636364 0.22727273]\n",
            "  [0.05       0.45       0.05       0.45      ]]]\n",
            "\n",
            "\n",
            "Normalized Probability matrix for policy:  [[0.14285714 0.28571429 0.25       0.32142857]\n",
            " [0.4        0.25       0.1        0.25      ]\n",
            " [0.2962963  0.33333333 0.33333333 0.03703704]\n",
            " [0.27272727 0.21212121 0.27272727 0.24242424]]\n"
          ]
        }
      ],
      "source": [
        "H = 1e6# defining the horizon for the MDP\n",
        "S = 4 # cardinality of State Space (We will replace this with SxA)\n",
        "A = 4 # cardinality of the Action Space\n",
        "SxA = S*A # cardinality of the Product Space of State and Actions\n",
        "\n",
        "# seeds = np.random.randint(10000, size=10)\n",
        "seeds = [42]\n",
        "np.random.seed(seeds[0]) # current seed set to 42\n",
        "\n",
        "current_state = np.random.randint(S) # initialize the current state randomly\n",
        "current_action = np.random.randint(A) # initialize the current state randomly\n",
        "\n",
        "# Now we generate a list of words that we will map onto the S X A space for unique representation of each S X A pair.\n",
        "# word_space = np.array(['the', 'to', 'of', 'in', 'and', 'he', 'is', 'for', 'on', 'said', 'that', 'has', 'says', 'was', 'have', 'it', 'be', 'are', 'with', 'will', 'at', 'mr', 'from', 'by', 'we', 'been', 'as', 'an', 'his', 'not', 'but', 'they', 'after', 'were', 'had', 'there', 'new', 'this', 'australian', 'australia', 'who', 'people', 'palestinian', 'their', 'two', 'government', 'up', 'us', 'south', 'which', 'year', 'one', 'about', 'out', 'if', 'also', 'more', 'when', 'its', 'into', 'would', 'first', 'against', 'last', 'israeli', 'minister', 'arafat', 'over', 'all', 'afghanistan', 'three', 'united', 'world', 'no', 'or', 'police', 'than', 'before', 'attacks', 'fire', 'day', 'security', 'some', 'states', 'you', 'could', 'them', 'today', 'say', 'now', 'told', 'time', 'any', 'laden', 'very', 'bin', 'just', 'can', 'company', 'what', 'president', 'sydney', 'still', 'man', 'four', 'killed', 'taliban', 'al', 'forces', 'our', 'around', 'days', 'being', 'west', 'old', 'other', 'where', 'so', 'officials', 'test', 'qaeda', 'israel', 'general', 'think', 'next', 'federal', 'per', 'force', 'she', 'cent', 'workers', 'leader', 'yesterday', 'state', 'take', 'under', 'hamas', 'him', 'bank', 'suicide', 'years', 'meeting', 'those', 'back', 'down', 'action', 'made', 'morning', 'commission', 're', 'international', 'pakistan', 'centre', 'city', 'attack', 'group', 'afghan', 'through', 'number', 'well', 'while', 'military', 'members', 'qantas', 'called', 'local', 'five', 'area', 'national', 'week', 'gaza', 'union', 'wales', 'hours', 'including', 'since', 'september', 'another', 'east', 'night', 'north', 'report', 'off', 'should', 'war', 'second', 'these', 'staff', 'between', 'go', 'earlier', 'get', 'six', 'defence', 'islamic', 'further', 'do', 'end', 'months', 'team', 'foreign', 'work', 'areas', 'because', 'going', 'sharon', 'power', 'authority', 'near', 'many', 'died', 'eight', 'only', 'way', 'during', 'india', 'know', 'month', 'northern', 'make', 'good', 'melbourne', 'former', 'air', 'spokesman', 'match', 'claims', 'left', 've', 'metres', 'prime', 'authorities', 'support', 'most', 'peace', 'like', 'osama', 'expected', 'given', 'set', 'saying', 'am', 'ago', 'looking', 'militants', 'come', 'bora', 'tora', 'put', 'place', 'several', 'children', 'fighters', 'found', 'unions', 'christmas', 'injured', 'arrested', 'groups', 'africa', 'troops', 'child', 'river', 'royal', 'meanwhile', 'indian', 'part', 'interim', 'yasser', 'official', 'whether', 'then', 'reports', 'hospital', 'terrorist', 'talks', 'economy', 'senior', 'statement', 'mountains', 'how', 'leaders', 'early', 'third', 'start', 'terrorism', 'don', 'industrial', 'hit', 'public', 'trying', 'family', 'court', 'pay', 'army', 'weather', 'believe', 'million', 'radio', 'both', 'john', 'however', 'adelaide', 'control', 'agreement', 'pressure', 'lead', 'long', 'dr', 'following', 'best', 'chief', 'asked', 'help', 'taken', 'want', 'does', 'better', 'overnight', 'few', 'play', 'high', 'service', 'arrest', 'firefighters', 'australians', 'need', 'services', 'queensland', 'confirmed', 'close', 'labor', 'process', 'house', 'community', 'detainees', 'information', 'came', 'secretary', 'opposition', 'believed', 'williams', 'must', 'won', 'possible', 'brought', 'win', 'nations', 'peter', 'her', 'british', 'did', 'hicks', 'released', 'governor', 'accused', 'shot', 'pentagon', 'held', 'damage', 'much', 'maintenance', 'party', 'took', 'conditions', 'such', 'york', 'without', 'council', 'building', 'violence', 'even', 'eastern', 'return', 'director', 'across', 'asylum', 'hill', 'got', 'cut', 'weekend', 'despite', 'kandahar', 'lot', 'airline', 'armed', 'safety', 'dead', 'change', 'winds', 'economic', 'country', 'home', 'men', 'strip', 'palestinians', 'working', 'trade', 'here', 'waugh', 'fires', 'david', 'lee', 'far', 'december', 'seekers', 'news', 'region', 'cricket', 'anti', 'board', 'monday', 'too', 'captured', 'crew', 'race', 'see', 'strong', 'role', 'call', 'training', 'fighting', 'emergency', 'continuing', 'southern', 'american', 'aircraft', 'offer', 'zinni', 'bush', 'george', 'plans', 'charged', 'industry', 'alliance', 'health', 'used', 'bureau', 'head', 'administration', 'water', 'received', 'rate', 'key', 'act', 'past', 'person', 'line', 'station', 'least', 'strikes', 'final', 'legal', 'known', 'town', 'decision', 'hih', 'large', 'issue', 'hundreds', 'israelis', 'your', 'within', 'use', 'major', 'risk', 'britain', 'leading', 'boat', 'stop', 'captain', 'soldiers', 'downer', 'operations', 'department', 'zealand', 'parliament', 'may', 'airport', 'interest', 'late', 'due', 'latest', 'might', 'pm', 'later', 'series', 'able', 'position', 'half', 'officers', 'kabul', 'homes', 'plane', 'laws', 'behind', 'shane', 'give', 'coast', 'death', 'western', 'taking', 'great', 'remain', 'tomorrow', 'hollingworth', 'right', 'policy', 'network', 'un', 'ahead', 'weapons', 'every', 'my', 'forced', 'event', 'hard', 'along', 'seen', 'deaths', 'concerned', 'same', 'towards', 'territory', 'storm', 'cup', 'victory', 'jobs', 'failed', 'really', 'me', 'bill', 'special', 'campaign', 'washington', 'point', 'already', 'jihad', 'heard', 'side', 'abuse', 'timor', 'flight', 'guilty', 'continue', 'thought', 'life', 'others', 'likely', 'detention', 'woomera', 'envoy', 'bombings', 'details', 'november', 'helicopters', 'sunday', 'weeks', 'situation', 'matter', 'launched', 'warne', 'countries', 'th', 'case', 'according', 'again', 'bichel', 'buildings', 'canyoning', 'capital', 'innings', 'mark', 'middle', 'hour', 'space', 'bus', 'seven', 'important', 'mcgrath', 'bombing', 'member', 'bowler', 'rates', 'cabinet', 'enough', 'move', 'cancer', 'boy', 'run', 'rule', 'warplanes', 'claimed', 'money', 'women', 'justice', 'jail', 'reported', 'african', 'immediately', 'ms', 'forward', 'raids', 'based', 'disease', 'added', 'office', 'guides', 'adventure', 'top', 'asio', 'targets', 'aedt', 'caught', 'blue', 'political', 'commonwealth', 'sure', 'show', 'swiss', 'wants', 'movement', 'evidence', 'mission', 'deal', 'young', 'dispute', 'carried', 'perth', 'own', 'human', 'freeze', 'opened', 'wicket', 'carrying', 'march', 'flying', 'certainly', 'job', 'find', 'border', 'each', 'investigation', 'result', 'break', 'although', 'burning', 'always', 'allegations', 'face', 'prepared', 'ground', 'sector', 'collapse', 'beat', 'using', 'island', 'reached', 'probably', 'proposed', 'full', 'growth', 'crash', 'times', 'planning', 'order', 'financial', 'access', 'become', 'banks', 'wave', 'reserve', 'outside', 'afp', 'bowling', 'senator', 'argentina', 'different', 'management', 'stage', 'needs', 'relations', 'program', 'surrender', 'militia', 'suspected', 'believes', 'caves', 'went', 'rejected', 'ariel', 'declared', 'making', 'alleged', 'travel', 'militant', 'ansett', 'road', 'howard', 'responsibility', 'bid', 'kilometres', 'harrison', 'future', 'energy', 'until', 'thousands', 'soon', 'short', 'organisation', 'sent', 'sex', 'tourists', 'calls', 'clear', 'trees', 'radical', 'killing', 'immigration', 'executive', 'allow', 'serious', 'flights', 'inside', 'fight', 'thursday', 'comes', 'quickly', 'opening', 'himself', 'inquiry', 'drop', 'post', 'getting', 'hewitt', 'lives', 'tried', 'july', 'yacht', 'issues', 'refused', 'destroyed', 'accident', 'anything', 'nine', 'karzai', 'alexander', 'gunmen', 'actually', 'bombers', 'quite', 'sources', 'parties', 'among', 'currently', 'agency', 'ministers', 'meet', 'available', 'warned', 'form', 'ended', 'white', 'rural', 'yet', 'parts', 'response', 'understand', 'open', 'done', 'residents', 'jewish', 'school', 'shortly', 'something', 'rights', 'changes', 'directors', 'terms', 'returned', 'list', 'tanks', 'figures', 'address', 'rise', 'sea', 'attempt', 'television', 'means', 'victoria', 'donald', 'source', 'running', 'try', 'wickets', 'kallis', 'sentence', 'announced', 'wounded', 'decided', 'biggest', 'look', 'offices', 'witnesses', 'happened', 'increase', 'fighter', 'brisbane', 'annual', 'measures', 'circumstances', 'ruddock', 'struck', 'friday', 'virus', 'began', 'rather', 'caused', 'tennis', 'whose', 'strike', 'attorney', 'huge', 'though', 'highway', 'islands', 'suharto', 'commissioner', 'wage', 'negotiations', 'car', 'elected', 'robert', 'runs', 'musharraf', 'period', 'having', 'intelligence', 'crowd', 'sir', 'airlines', 'powell', 'confidence', 'worst', 'business', 'market', 'beginning', 'arrived', 'blake', 'recession', 'big', 'supporters', 'rafter', 'glenn', 'allan', 'field', 'afternoon', 'difficult', 'ensure', 'prevent', 'expect', 'involved', 'gives', 'deputy', 'bomb', 'ses', 'facility', 'appeared', 'followed', 'commanders', 'gave', 'advice', 'step', 'less', 'media', 'total', 'expressed', 'nauru', 'pacific', 'hobart', 'away', 'fact', 'commander', 'tour', 'cost', 'ever', 'll', 'beyond', 'law'])\n",
        "# We generate state names as s0_0, s0_1, s0_2, s0_3, s1_0 ... with si_j where 1<=i<=|S|-1, 1<=j<=|A|-1\n",
        "word_space = []\n",
        "for i in range(S):\n",
        "    for j in range(A):\n",
        "        word_space.append('s'+str(i)+'_'+str(j))\n",
        "    # end for j\n",
        "# end for i\n",
        "print(\"Length of word_space i.e. |S|x|A| = \", len(word_space))\n",
        "print(\"List of words: \", word_space)\n",
        "\n",
        "# Please note that right now the word_space length is only 889, hence if SXA>889, then we can't run the mapping as of now.\n",
        "# However that's a trivial issue which will be fixed soon (by adding more words from a larger dictionary)\n",
        "\n",
        "states = np.array(word_space[0:S*A]).reshape(S,A) # this stores the word equivalent of each state (state-action pair).\n",
        "traj = [states[current_state, current_action]] # stores the states observed in a list that we use to generate the .txt file\n",
        "\n",
        "P_ini = np.random.randint(10, size=(S,A,S)) # Transition Probability Matrix without normalizing each row\n",
        "# print(P_ini)\n",
        "\n",
        "# For a 3-D Probability matrix, we peform the following steps\n",
        "P = np.empty(P_ini.shape)\n",
        "for index in range(S):\n",
        "    Pa = P_ini[index,:,:]\n",
        "    # print(\"Initial matrix for transitions:\", Pa)\n",
        "    normalizer = np.sum(Pa, axis=1) # Denominator of each row\n",
        "    Pa = (Pa.transpose()/normalizer).transpose()\n",
        "    # print(\"Normalized Probability matrix for transitions: \", Pa)\n",
        "    P[index,:,:] = Pa\n",
        "    # print(\"\\n\")\n",
        "\n",
        "print(\"Final Probability matrix: \", P)\n",
        "print(\"\\n\")\n",
        "# P now stores the transition probability matrix for the states given (s,a)\n",
        "\n",
        "'''\n",
        "Now to obtain the policy i.e. the probability distribution over |A|\n",
        "'''\n",
        "# Now we need to create a probability matrix to choose an action from A given the current state = s.\n",
        "# P_a = np.random.randint(10, size=(S,A))\n",
        "P_a = np.random.randint(1, 10, size=(S,A)) # this ensures that each action has a non-zero probability given s\n",
        "# print(\"Initial Probability matrix for policy:\", P_a)\n",
        "normalizer = np.sum(P_a, axis=1) # Denominator of each row\n",
        "P_a = (P_a.transpose()/normalizer).transpose()\n",
        "print(\"Normalized Probability matrix for policy: \", P_a)\n",
        "\n",
        "# We generate random rewards and normalize them to ensure 0<= r(s,a) <=1\n",
        "rewards = np.random.randint(10, size=(S,A))/10 # this stores the reward corresponding to each state action pair\n",
        "# print(\"reward matrix: \", rewards)\n",
        "\n",
        "# assert False, \"Break check!\"\n",
        "\n",
        "# # Now we save the Transition Probabilities for comparison with Linear MDP\n",
        "# temp_filename_P = \"%i_states_%i_actions_transition_probability_matrix\" %(S,A)\n",
        "# temp_filename_a = \"%i_states_%i_actions_action_probability_matrix\" %(S,A)\n",
        "# temp_filename_r = \"%i_states_%i_actions_rewards_matrix\" %(S,A)\n",
        "\n",
        "# np.save(temp_filename_P, P)\n",
        "# np.save(temp_filename_a, P_a)\n",
        "# np.save(temp_filename_r, rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EaVqIV9SL5H",
        "outputId": "7ea0253d-adb7-4d32-bba9-52bcad39c2c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 999999/999999 [01:17<00:00, 12820.73it/s]\n"
          ]
        }
      ],
      "source": [
        "# H = 10\n",
        "for t in tqdm(np.arange(1,H)):\n",
        "    next_state = np.random.choice(np.arange(S), 1, p = P[current_state,current_action,:]) # to generate index for next state\n",
        "    current_state = next_state[0]\n",
        "    next_action = np.random.choice(np.arange(A), 1, p = P_a[current_state,:]) # to generate index for next action\n",
        "    current_action = next_action[0]\n",
        "    traj.append(states[current_state, current_action])\n",
        "\n",
        "# print(\"Trajectory: \", traj)\n",
        "# # Now we create the string version of the trajectory to provide as input to the word2vec algo\n",
        "# trajectory = ' '.join(str(x) for x in traj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6SCHYNESNXX"
      },
      "source": [
        "## word2vec code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hu8FGIOsHOY",
        "outputId": "e5b69566-5f8a-4e74-a3f6-f1733ed647ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 337783.3059\n",
            "Epoch 2, Loss: 337732.9812\n",
            "Epoch 3, Loss: 337743.2871\n",
            "Epoch 4, Loss: 337740.6112\n",
            "Epoch 5, Loss: 337762.4215\n",
            "Epoch 6, Loss: 337800.4208\n",
            "Epoch 7, Loss: 337736.1856\n",
            "Epoch 8, Loss: 337770.4150\n",
            "Epoch 9, Loss: 337772.8981\n",
            "Epoch 10, Loss: 337753.2215\n",
            "Updated embedding for 's0_0': [[ 0.04957308 -0.3981189  -0.1694536  -1.9118497 ]]\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Using word2vec as is doesn't work because:\n",
        "    1. We have a continuous state space\n",
        "    2. We don't have a way to show same states again. This is a problem in training w2v since we need multiple\n",
        "    context samples for each word else we can't train the w2v model. Thus, we need a rule to generate several\n",
        "    positive examples for each state that we see in the trajectory\n",
        "'''\n",
        "# # All the imports\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import numpy as np\n",
        "# from collections import Counter\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define a function to convert trajectories into text\n",
        "\n",
        "embed_size = S\n",
        "# Start defining word2vec as provided by ChatGPT-4o\n",
        "def tokenize_text(text): # This function isn't needed since the trajectory is already in the form of tokens\n",
        "    return text.lower().split()\n",
        "\n",
        "def build_vocab(text): # Again we already have a vocabulary hence don't need to use this function directly\n",
        "    # words = tokenize_text(text)\n",
        "    # words = text\n",
        "    # word_counts = Counter(words)\n",
        "    word_counts = Counter(text)\n",
        "    vocab = {word: i for i, word in enumerate(word_counts.keys())}\n",
        "    reverse_vocab = {i: word for word, i in vocab.items()}\n",
        "    return vocab, reverse_vocab, word_counts\n",
        "\n",
        "def generate_skipgram_pairs(text, window_size=2): # This function gives the word and context pairs.\n",
        "    # words = tokenize_text(text)\n",
        "    words = text\n",
        "    pairs = []\n",
        "    for i, target_word in enumerate(words):\n",
        "        window_start = max(i - window_size, 0)\n",
        "        window_end = min(i + window_size + 1, len(words))\n",
        "        for j in range(window_start, window_end):\n",
        "            if i != j:\n",
        "                pairs.append((words[i], words[j]))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "class Word2VecDataset(Dataset):\n",
        "    def __init__(self, text, vocab, window_size=2, num_neg_samples=5):\n",
        "        self.vocab = vocab\n",
        "        self.data = generate_skipgram_pairs(text, window_size)\n",
        "        self.num_neg_samples = num_neg_samples\n",
        "        self.vocab_size = len(vocab)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        target, context = self.data[idx] # How does this idx reflect the actual idx of the word?\n",
        "        ''' Doesn't the above idx reflect the idx numbered pair instead in pairs instead of the pairs\n",
        "        corresponding to the word at idx? '''\n",
        "        target_idx = torch.tensor(self.vocab[target], dtype=torch.long)\n",
        "        context_idx = torch.tensor(self.vocab[context], dtype=torch.long)\n",
        "\n",
        "        # Negative Sampling: Random words sampled (excluding actual context word)\n",
        "        negative_samples = torch.randint(0, self.vocab_size, (self.num_neg_samples,))\n",
        "        while context_idx in negative_samples:\n",
        "            negative_samples = torch.randint(0, self.vocab_size, (self.num_neg_samples,))\n",
        "\n",
        "        return target_idx, context_idx, negative_samples\n",
        "\n",
        "\n",
        "# Building the skip-gram network\n",
        "class SkipGramNegativeSampling(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, precomputed_vectors=None):\n",
        "        super(SkipGramNegativeSampling, self).__init__()\n",
        "\n",
        "        # Create an embedding layer\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # Initialize embeddings with precomputed word vectors (if available)\n",
        "        if precomputed_vectors is not None:\n",
        "            self.embeddings.weight.data.copy_(torch.tensor(precomputed_vectors, dtype=torch.float32))\n",
        "\n",
        "        # Output layer (to predict context words)\n",
        "        self.output_layer = nn.Linear(embed_size, vocab_size, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, target_idx):\n",
        "        target_vec = self.embeddings(target_idx)  # Get word embeddings\n",
        "        output = self.output_layer(target_vec)  # Compute logits for context words\n",
        "        final_output = self.sigmoid(output)  # Apply sigmoid to get probabilities\n",
        "        return final_output\n",
        "\n",
        "    def loss_function(self, pos_scores, neg_scores):\n",
        "        \"\"\"\n",
        "        Implements negative sampling loss.\n",
        "        pos_scores: Scores for true context words\n",
        "        neg_scores: Scores for sampled negative words\n",
        "        \"\"\"\n",
        "        pos_loss = torch.log(self.sigmoid(pos_scores)).mean()\n",
        "        neg_loss = torch.log(1 - self.sigmoid(neg_scores)).mean()\n",
        "        return -(pos_loss + neg_loss)  # Negative log likelihood\n",
        "\n",
        "\n",
        "# Create the training loop:\n",
        "def train_model(text, precomputed_vectors, embed_size=embed_size, window_size=2, epochs=10, batch_size=16, lr=0.01, num_neg_samples=5):\n",
        "    vocab, reverse_vocab, _ = build_vocab(text)\n",
        "    dataset = Word2VecDataset(text, vocab, window_size, num_neg_samples)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Convert precomputed vectors into a matrix (shape: vocab_size x embed_size)\n",
        "    word_vector_matrix = np.zeros((len(vocab), embed_size))\n",
        "    for word, idx in vocab.items():\n",
        "        # word_vector_matrix[idx] = precomputed_vectors.get(word, np.random.randn(embed_size))  # Use random if not found\n",
        "        word_vector_matrix[idx] = np.random.randn(embed_size)\n",
        "        # assert False, \"Check above line for the .get() function\"\n",
        "\n",
        "    model = SkipGramNegativeSampling(len(vocab), embed_size, word_vector_matrix)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for target_idx, context_idx, negative_samples in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            predictions = model(target_idx)  # Logits (before sigmoid)\n",
        "            pos_scores = predictions.gather(1, context_idx.unsqueeze(1)).squeeze(1)  # Get true context scores\n",
        "            neg_scores = predictions.gather(1, negative_samples).mean(dim=1)  # Get negative samples scores\n",
        "\n",
        "            # Compute loss\n",
        "            loss = model.loss_function(pos_scores, neg_scores)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "    return model, vocab # this ensures that we can use the vocab created here\n",
        "\n",
        "###################################################################################################### Example #######################\n",
        "# Sample text and precomputed word vectors\n",
        "# word_vectors = {word: np.random.randn(embed_size) for word in [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\"]}  # Dummy vectors\n",
        "# text = \"the quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Train model\n",
        "text = traj\n",
        "word_vectors = []\n",
        "model, vocab = train_model(text, word_vectors) # word_vectors corresponds to the precomputed-vectors for each word\n",
        "\n",
        "# Get the learned embedding for a word\n",
        "word = \"s0_0\"\n",
        "if word in vocab:  # âœ… Ensure word exists in vocab\n",
        "    word_idx = torch.tensor([vocab[word]], dtype=torch.long)\n",
        "    updated_embedding = model.embeddings(word_idx).detach().numpy()\n",
        "    print(f\"Updated embedding for '{word}': {updated_embedding}\")\n",
        "else:\n",
        "    print(f\"Word '{word}' not found in vocabulary.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHeLvY3rXsA4"
      },
      "source": [
        "## Q-learning code starts here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyXwe9jNfWXd"
      },
      "source": [
        "Defining the QLearningAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hQAYBzyGHRYw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, num_states, num_actions, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.q_table = np.zeros((num_states, num_actions))  # Initialize Q-table\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.randint(0, self.num_actions - 1)  # Explore\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state, :])  # Exploit\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        \"\"\"Q-learning update rule.\"\"\"\n",
        "        best_next_action = np.argmax(self.q_table[next_state, :])  # Greedy action for next state\n",
        "        td_target = reward + self.gamma * self.q_table[next_state, best_next_action]\n",
        "        td_error = td_target - self.q_table[state, action]\n",
        "        self.q_table[state, action] += self.alpha * td_error  # Update Q-table\n",
        "\n",
        "    # To compute the Optimal Policy of the given MDP\n",
        "    def get_optimal_policy(self):\n",
        "        \"\"\"Extracts the optimal policy after training.\"\"\"\n",
        "        optimal_policy = np.argmax(self.q_table, axis=1)\n",
        "        return optimal_policy\n",
        "\n",
        "    # Printing the final Q-table obtained\n",
        "    def print_q_table(self):\n",
        "        \"\"\"Prints the learned Q-table for debugging.\"\"\"\n",
        "        print(\"Learned Q-table:\")\n",
        "        print(self.q_table)\n",
        "\n",
        "    # Printing the optimal policy after completion\n",
        "    def print_optimal_policy(self):\n",
        "        \"\"\"Prints the optimal policy in a readable format.\"\"\"\n",
        "        optimal_policy = self.get_optimal_policy()\n",
        "        print(\"Optimal Policy (best action for each state):\")\n",
        "        for state in range(self.num_states):\n",
        "            print(f\"State {state}: Take Action {optimal_policy[state]}\")\n",
        "\n",
        "\n",
        "###########################################################\n",
        "# Defining additional functions useful for policy checking\n",
        "###########################################################\n",
        "def check_policy_stability(agent, prev_policy):\n",
        "    \"\"\"Returns True if the policy is stable (no change in optimal actions).\"\"\"\n",
        "    current_policy = agent.get_optimal_policy()\n",
        "    return np.array_equal(prev_policy, current_policy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W01GL0GNhzmw"
      },
      "source": [
        "Policy stabilization based stopping criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bTE7HYyAhz1F"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prev_policy \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_optimal_policy()\n\u001b[1;32m      2\u001b[0m stable_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Number of episodes with stable policy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m stable_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# Stop if policy remains unchanged for 10 episodes\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "prev_policy = agent.get_optimal_policy()\n",
        "stable_count = 0  # Number of episodes with stable policy\n",
        "stable_threshold = 10  # Stop if policy remains unchanged for 10 episodes\n",
        "\n",
        "if check_policy_stability(agent, prev_policy):\n",
        "    stable_count += 1\n",
        "else:\n",
        "    stable_count = 0  # Reset count if policy changes\n",
        "\n",
        "if stable_count >= stable_threshold:\n",
        "    print(f\"Policy stabilized for {stable_threshold} episodes. Stopping training.\")\n",
        "    break\n",
        "\n",
        "prev_policy = agent.get_optimal_policy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay-FZzmwh_Dl"
      },
      "source": [
        "Q-value not changing based stopping criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vQZ8vlC2iC2N"
      },
      "outputs": [],
      "source": [
        "def run_q_learning(env, agent, num_episodes=1000, convergence_threshold=1e-4):\n",
        "    \"\"\"Train the agent and check for Q-value convergence.\"\"\"\n",
        "    prev_q_table = np.copy(agent.q_table)  # Store old Q-table\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.update_q_value(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        # Compute max Q-value change\n",
        "        q_change = np.max(np.abs(agent.q_table - prev_q_table))\n",
        "        prev_q_table = np.copy(agent.q_table)\n",
        "\n",
        "        if q_change < convergence_threshold:\n",
        "            print(f\"Converged at Episode {episode+1} with max Q-change: {q_change}\")\n",
        "            break\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            print(f\"Episode {episode+1}: Max Q-value change = {q_change}\")\n",
        "\n",
        "    print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QNFhVQhfctt"
      },
      "source": [
        "Using the above MDP for generating the next state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8iva2VHfV-d"
      },
      "outputs": [],
      "source": [
        "seeds = [42]\n",
        "np.random.seed(seeds[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8cB-OuHfPA9"
      },
      "source": [
        "### For Frozen Lake or other gymnasium env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlKvX3zEfOo-"
      },
      "outputs": [],
      "source": [
        "def run_q_learning(env, agent, num_episodes=1000):\n",
        "    \"\"\"Train the agent in the given environment.\"\"\"\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()  # Reset environment at the start of each episode\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)  # Interact with environment\n",
        "            agent.update_q_value(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            print(f\"Episode {episode+1}: Total Reward = {total_reward}\")\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "# Example usage:\n",
        "# Define an environment following OpenAI Gym API with `reset()` and `step(action)`.\n",
        "# For example, `env = gym.make(\"FrozenLake-v1\")`\n",
        "# agent = QLearningAgent(num_states=env.observation_space.n, num_actions=env.action_space.n)\n",
        "# run_q_learning(env, agent, num_episodes=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj-7z5Rdj-l2"
      },
      "source": [
        "# ChatGPT code\n",
        "Here is an implementation of generating the MDP as an environment with step() as a function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-YSMluS5kBdG"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "'p' must be 1-dimensional",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Create and train the Q-learning agent\u001b[39;00m\n\u001b[1;32m    116\u001b[0m agent \u001b[38;5;241m=\u001b[39m QLearningAgent(num_states\u001b[38;5;241m=\u001b[39mS, num_actions\u001b[38;5;241m=\u001b[39mA)\n\u001b[0;32m--> 117\u001b[0m trained_agent, reward_history \u001b[38;5;241m=\u001b[39m run_q_learning(env, agent)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Display learned Q-table and optimal policy\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[2], line 76\u001b[0m, in \u001b[0;36mrun_q_learning\u001b[0;34m(env, agent, num_episodes, convergence_threshold, stable_threshold)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     75\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[0;32m---> 76\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     77\u001b[0m     agent\u001b[38;5;241m.\u001b[39mupdate_q_value(state, action, reward, next_state)\n\u001b[1;32m     78\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
            "Cell \u001b[0;32mIn[2], line 30\u001b[0m, in \u001b[0;36mRandomMDP.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Performs an action, returning (next_state, reward, done, info).\"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mS, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, action])\n\u001b[1;32m     31\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, action]\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m next_state\n",
            "File \u001b[0;32mnumpy/random/mtrand.pyx:966\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'p' must be 1-dimensional"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the randomly generated MDP parameters\n",
        "S = 4  # Number of states\n",
        "A = 4  # Number of actions\n",
        "\n",
        "# Random transition probabilities (S x A x S)\n",
        "P = np.random.dirichlet(np.ones(S), size=(S, A))\n",
        "\n",
        "# Random rewards (S x A)\n",
        "r = np.random.uniform(-1, 1, (S, A))\n",
        "\n",
        "class RandomMDP:\n",
        "    \"\"\"Simulated environment for a randomly generated MDP.\"\"\"\n",
        "    def __init__(self, S, A, P, r):\n",
        "        self.S = S\n",
        "        self.A = A\n",
        "        self.P = P  # Transition probabilities\n",
        "        self.r = r  # Reward function\n",
        "        self.state = np.random.randint(0, S)  # Initial state\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the environment and returns the initial state.\"\"\"\n",
        "        self.state = np.random.randint(0, S)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Performs an action, returning (next_state, reward, done, info).\"\"\"\n",
        "        next_state = np.random.choice(self.S, p=self.P[self.state, action])\n",
        "        reward = self.r[self.state, action]\n",
        "        self.state = next_state\n",
        "        done = False  # Assume episode never ends (for simplicity)\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "class QLearningAgent:\n",
        "    \"\"\"Q-learning agent.\"\"\"\n",
        "    def __init__(self, num_states, num_actions, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.q_table = np.zeros((num_states, num_actions))  # Initialize Q-table\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.randint(0, self.num_actions - 1)  # Explore5000\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        \"\"\"Q-learning update rule.\"\"\"\n",
        "        best_next_action = np.argmax(self.q_table[next_state, :])  # Greedy action for next state\n",
        "        td_target = reward + self.gamma * self.q_table[next_state, best_next_action]\n",
        "        td_error = td_target - self.q_table[state, action]\n",
        "        self.q_table[state, action] += self.alpha * td_error  # Update Q-table\n",
        "\n",
        "    def get_optimal_policy(self):\n",
        "        \"\"\"Extracts the optimal policy after training.\"\"\"\n",
        "        return np.argmax(self.q_table, axis=1)\n",
        "\n",
        "def run_q_learning(env, agent, num_episodes=1, convergence_threshold=1e-4, stable_threshold=10):\n",
        "    \"\"\"Train the agent and check for Q-value and policy convergence.\"\"\"\n",
        "    prev_q_table = np.copy(agent.q_table)  # Store old Q-table\n",
        "    prev_policy = agent.get_optimal_policy()\n",
        "    stable_count = 0  # Number of episodes with stable policy\n",
        "    reward_history = []\n",
        "    episode_count = 0\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.update_q_value(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            reward_history.append(total_reward)\n",
        "\n",
        "            # Compute max Q-value change\n",
        "            q_change = np.max(np.abs(agent.q_table - prev_q_table))\n",
        "            prev_q_table = np.copy(agent.q_table)\n",
        "\n",
        "            # Check Q-value convergence\n",
        "            if q_change < convergence_threshold:\n",
        "                print(f\"Q-values converged at Episode {episode+1} with max Q-change: {q_change}\")\n",
        "                break\n",
        "\n",
        "            # Check policy stability\n",
        "            current_policy = agent.get_optimal_policy()\n",
        "            if np.array_equal(prev_policy, current_policy):\n",
        "                stable_count += 1\n",
        "            else:\n",
        "                stable_count = 0  # Reset count if policy changes\n",
        "\n",
        "            prev_policy = current_policy\n",
        "\n",
        "            if stable_count >= stable_threshold:\n",
        "                print(f\"Policy stabilized for {stable_threshold} episodes. Stopping training at Episode {episode+1}.\")\n",
        "                break\n",
        "            \n",
        "            episode_count+=1\n",
        "            if episode_count>1000:\n",
        "                break\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    return agent, reward_history\n",
        "\n",
        "# Create a random MDP environment\n",
        "env = RandomMDP(S, A, P, r)\n",
        "\n",
        "# Create and train the Q-learning agent\n",
        "agent = QLearningAgent(num_states=S, num_actions=A)\n",
        "trained_agent, reward_history = run_q_learning(env, agent)\n",
        "\n",
        "# Display learned Q-table and optimal policy\n",
        "import pandas as pd\n",
        "# import ace_tools as tools\n",
        "\n",
        "q_table_df = pd.DataFrame(trained_agent.q_table, columns=[f'Action {a}' for a in range(A)])\n",
        "q_table_df.index.name = 'State'\n",
        "\n",
        "policy_df = pd.DataFrame({'Optimal Action': trained_agent.get_optimal_policy()})\n",
        "policy_df.index.name = 'State'\n",
        "\n",
        "# Display results\n",
        "# tools.display_dataframe_to_user(name=\"Learned Q-Table\", dataframe=q_table_df)\n",
        "# tools.display_dataframe_to_user(name=\"Optimal Policy\", dataframe=policy_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_NcduVLeShy"
      },
      "outputs": [],
      "source": [
        "print(\"Q - Table: \", trained_agent.q_table)\n",
        "print(\"Optimal Policy: \", trained_agent.get_optimal_policy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB3WeT9menKO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
